{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Introduction (Page 2)-\\n\\nThe paper proposes a new model that leverages the strengths of both paradigms. The ultimate goal seems to be the development of an \"omni-model\" capable of handling various tasks, including those requiring limited data. By combining pre-training for general knowledge with prompt/instruction tuning for adaptation to specific tasks, the model can potentially be versatile and efficient across a wide range of NLP applications.\\n\\nProperties of omnipotent model:-\\nTask agnostic - means it works for a variety of tasks without needing specific modifications for each one. Eg. classification, generation, self-supervised pretext tasks, etc., and to be agnostic to either pre training or fine tuning\\nModel Agnostic - that can be applied to different machine learning models, regardless of the underlying architecture.\\nTask Comprehensiveness - enough task variety to accumulate generalization ability robustly.\\n\\n\\nThe research paper explores building a powerful and versatile model for various natural language processing (NLP) tasks.Here\\'s a summary of the key points:\\nUnified Training Approach: The model uses a single framework based on sequence-to-sequence (Seq2eq) learning for both pre-training (initial training on a massive dataset) and fine-tuning (adapting to specific tasks). This simplifies training and potentially improves transfer learning between tasks.\\nModality Agnostic Transformer: The model leverages a Transformer architecture to process information,regardless of whether it\\'s text or image data. This is achieved through a shared vocabulary where both text and image data are converted into numerical representations.\\nTask Agnostic with Instructions: While the core structure remains Seq2Seq, the model uses handcrafted instructions for specific tasks within this framework. These instructions likely guide the model towards the desired outcome for each task.\\nTask Comprehensiveness: By pretraining on a variety of tasks, including those involving text only (unimodal) and those requiring understanding relationships between text and images (cross-modal), the model gains a broader understanding of different data types and their connections.\\n\\nI/O :-\\nSimilar to GPT and BART models, the text data undergoes Byte Pair Encoding (BPE) to transform it into a sequence of subwords. These subwords are then embedded into numerical feature vectors, allowing the model to represent the textual information.\\nThe ResNet modules act as feature extractors. They convert the raw image data (represented as a 3D tensor with Height,Width, and Channels) into a sequence of patch features. These patch features capture information from specific regions within the image and are also converted into numerical feature vectors.\\nImage: Images traditionally require different representations than text. The approach here suggests discretizing the image data, essentially converting it into a sequence of tokens from the unified vocabulary.\\nThis discretization likely involves techniques from recent advancements in image quantization. Image quantization reduces the complexity of an image by converting it into a series of discrete codes. These codes can then be mapped to tokens within the unified vocabulary.\\nObjects : If the task involves objects, they might also be represented using tokens from the unified vocabulary. This could involve assigning a unique token to each object category.\\n\\nBenefits of Discretization and Sparse Coding:\\nThe passage mentions two techniques used for image discretization:\\nImage Quantization: As mentioned earlier, this reduces the complexity of the image data by converting it into a series of codes. This compressed representation can be more efficient for the model to process.\\nSparse Coding: This technique represents the image using a sparse code sequence. Sparse means that most of the codes in the sequence will be zero, with only a few non-zero values representing the most important features of the image. This can significantly reduce the length of the sequence compared to the original image data, further improving efficiency.\\nSince the model uses a single vocabulary for everything, objects are also represented as a sequence of tokens. For each object, the model extracts its name (using BPE for text) and its location in the image (by converting the corner positions of its bounding box into a series of numbers). This allows the model to understand both what the object is and where it\\'s located in the image.\\nARCHITECTURE :-\\nEncoder:\\nSelf-Attention in the Encoder: Here, self-attention is likely applied independently to both the processed text and image data (represented as sequences).\\nFor the text sequence: Self-attention allows the model to understand how words in the sentence relate to each other, capturing the grammatical structure and meaning of the sentence.\\nFor the image sequence (patches): Self-attention helps the model understand how different parts of the image (represented by patches) relate to each other, identifying spatial relationships and recognizing objects within the image.\\nThis separate application of self-attention allows the model to learn internal relationships within each data type (text and image) before attempting to connect them.\\nDecoder :\\nCross-Attention: This is where cross-attention comes into play. The decoder uses the output from the encoder (processed text and image information) to generate the final output (e.g., image caption).\\nThe decoder employs cross-attention to focus on relevant parts of the encoded text sequence in relation to the parts of the encoded image sequence during the generation process. This allows the model to create an output (like a caption) that aligns with the information in both the image and the text.\\nIn essence, self-attention helps the model understand the internal structure of each data type (text and image) separately,while cross-attention allows it to bridge the gap between the two modalities, enabling tasks like image captioning.\\n\\nUnified Sequence-to-Sequence (Seq2Seq) Paradigm:\\nProposed a single framework based on Seq2Seq learning for all tasks, regardless of the data types involved (text, image, or a combination). This means both the pretraining stage (initial training on a massive dataset) and the fine-tuning stage (adapting to specific tasks) follow the Seq2Seq structure.\\nBenefits of a Unified Approach:\\nSimpler Training Process: Having a single framework simplifies the training process and reduces the need for separate approaches for different tasks or data types.\\nTransfer Learning: The knowledge learned during pre training on various tasks can be more easily transferred to new, unseen tasks during fine-tuning.\\nSeq2Seq for All Tasks:\\nThe text mentions that both pre-training tasks and downstream tasks (tasks the model is ultimately used for) are formulated as Seq2Seq generation.\\nSeq2Seq: In Seq2Seq models, you provide an input sequence (text or another data type) and the model generates a new output sequence. For instance, machine translation takes a sentence in one language (input sequence) and generates the corresponding translation in another language (output sequence).\\nApplying Seq2Seq Here: The paper likely defines specific ways to frame various tasks (like image captioning or text summarization) as input and output sequences within the Seq2Seq framework.\\nMultitask Pre Training on Multiple Data Types:\\nThis unified approach allows the model to be pre-trained on a massive dataset containing various data types (multimodal data like text and image, or even unimodal data like text only). This multitask pretraining helps the model develop a comprehensive understanding of different data types and how they might relate to each other.\\nEndowing the Model with Comprehensive Capabilities: By training on diverse tasks and data types, the model gains a broader range of abilities, making it more versatile for various applications.\\nShared Schema with Handcrafted Instructions:\\nThe model uses the same underlying structure (schema) across all tasks. This reinforces the concept of a unified approach.\\nHowever, for some tasks (like discrimination tasks that involve distinguishing between categories), the paper mentions using handcrafted instructions. These instructions likely provide additional guidance to the model specific to the discrimination task, even though the overall framework remains Seq2Seq.\\n\\nTraining & Inference :-\\nTraining with Cross-Entropy Loss:\\nThe model is trained using a common optimization technique called cross-entropy loss. This function measures the difference between the predicted probability distribution (how likely the model thinks each possible output is) and the actual correct output.\\nInference with Decoding Strategies:\\nInference refers to using the trained model to make predictions on new, unseen data.\\nThe text mentions using decoding strategies like beam search to improve the quality of the generated output (text or labels in this case). Beam search is a technique that helps explore a wider range of possible outputs during generation and choose the most likely one.\\nLimitations in Classification Tasks:\\nTwo main issues with the current approach when applied to classification tasks (tasks where the model needs to assign a predefined label to an input):\\nUnnecessary and Inefficient Optimization: During training, the model optimizes for the entire vocabulary.This means it considers all possible words or labels, even those not relevant to the specific classification task.This can be inefficient and computationally expensive.\\nGenerating Invalid Labels: During inference, the model might generate labels that are not even part of the pre-defined set of possible labels for the classification task. This can lead to errors.\\nTrie-Based Search for Classification (Solution):\\nTo address these limitations, the authors propose a new approach called Trie-based search. A Trie (also called a prefix tree) is a data structure that efficiently stores and retrieves words or labels based on their prefixes.\\nThe details of how the Trie is used , the idea is that the Trie can guide the model towards valid labels during inference, improving its performance in classification tasks.\\n\\nText to Image Generation :-\\nChallenge of Text-to-Image Generation:\\nGenerating images from text descriptions is a complex task for any model, even pre-trained ones.\\nOFA\\'s Pretraining and Code Generation:\\nThe paper proposes a different approach:\\nDuring pretraining, OFA is trained on a task called \"image-infilling.\" This involves recovering missing patches (masked areas) in images by generating the corresponding codes.\\nThese codes likely represent a compressed version of the image data, capturing the essential features of the missing patches.\\nBy successfully performing image-infilling, OFA essentially learns how to generate these image codes.\\nFine-tuning for Text-to-Code Generation:\\nOnce pre-trained on image-infilling, OFA is then fine-tuned on a dataset like MSCOCO Image Caption. This dataset likely pairs textual descriptions (captions) of images with the corresponding images.\\nDuring fine-tuning, OFA learns how to map the text captions to the image codes it can generate.\\nInference and Code Decoder:\\nAt inference time (when using the trained model to generate images), the process involves two steps:\\nText to Code: OFA takes a text description (query text) as input and generates the corresponding image codes based on its fine-tuned knowledge.\\nCode to Image: A separate decoder, likely based on a pre-trained model like VQGAN (mentioned in the passage), takes the generated codes and transforms them back into an actual image.\\nStrengths :-\\nStrengths of the OFA Model (as described in the research paper):\\nUnified Approach: OFA utilizes a single framework based on sequence-to-sequence (Seq2Seq) learning for various tasks, simplifying training and potentially improving transfer learning between tasks.\\nModality Agnostic: The model\\'s core architecture can handle different data types (text and image) without extensive modifications for each task.\\nTask Agnostic with Instructions: While the core structure remains Seq2Seq, the model can be guided by handcrafted instructions for specific tasks within this framework.\\nTask Comprehensiveness: Pretraining on diverse tasks, including multimodal and unimodal ones, equips the model with a broader understanding of different data types and their relationships.\\nTrie-based Search for Classification: This approach improves accuracy and efficiency in classification tasks by guiding the model towards valid labels during inference.\\nText-to-Image Generation with Codes: Leveraging image code generation learned during pretraining allows for efficient processing and potentially better control over image generation.\\nSmaller Sampling Size: OFA achieves good results on text-to-image generation with a smaller sampling size compared to other methods, suggesting better efficiency and lower computational requirements.\\nWeaknesses of OFA:\\n1. Limited Performance in Sentence-Pair Classification:\\nThe passage mentions that both OFA and a compared model (Uni-Perceiver) achieve accuracy below 60% in sentence-pair classification tasks. This suggests a weakness in handling tasks that require comparing and reasoning about two different sentences.\\n2. Sensitivity to Instruction Design:\\nThe model\\'s performance is highly dependent on the design of handcrafted instructions used within the Seq2Seq framework. Finding the optimal instruction template requires searching through a large pool of candidates, which can be time-consuming and resource-intensive.\\nEven small changes to these instructions or model parameters can significantly impact performance, making the model less robust and potentially less reliable for real-world applications.\\n', metadata={'source': 'ofa.txt'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"ofa.txt\")\n",
    "text = loader.load()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"langchain_api_key\"] = \"langchain_api_key=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 \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "web_loader = WebBaseLoader(web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                           bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                               class_ = (\"post-title\",\"pose-content\",\"post-header\")\n",
    "                           )))\n",
    "text_document_2 = web_loader.load()\n",
    "text_document_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='OFA: U NIFYING ARCHITECTURES , TASKS ,AND MODALITIES\\nTHROUGH A SIMPLE SEQUENCE -TO-SEQUENCE LEARNING\\nFRAMEWORK\\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai\\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang\\nDAMO Academy, Alibaba Group∗\\n{zheluo.wp, ya235025, menrui.mr, junyang.ljy, baishuai.bs,\\nzhikang.lzk, jason.mjx, ericzhou.zc, jingren.zhou, yang.yhx}@alibaba-inc.com\\nFigure 1: Examples of various tasks supported by OFA.\\nABSTRACT\\nIn this work, we pursue a uniﬁed paradigm for multimodal pretraining to break the scaffolds of\\ncomplex task/modality-speciﬁc customization. We propose OFA, a Task-Agnostic and Modality-\\nAgnostic framework that supports Task Comprehensiveness. OFA uniﬁes a diverse set of cross-\\nmodal and unimodal tasks, including image generation, visual grounding, image captioning, image\\nclassiﬁcation, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA\\nfollows the instruction-based learning in both pretraining and ﬁnetuning stages, requiring no extra\\ntask-speciﬁc layers for downstream tasks. In comparison with the recent state-of-the-art vision\\n& language models that rely on extremely large cross-modal datasets, OFA is pretrained on only\\n20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training\\ndata, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive\\nperformances on uni-modal tasks. Our further analysis indicates that OFA can also effectively\\ntransfer to unseen tasks and unseen domains. Our code and models are publicly available at https:\\n//github.com/OFA-Sys/OFA .\\nKeywords Uniﬁed frameworks ·Multimodal pretraining ·Multitask learning ·Zero-shot learning\\n∗Correspondence to: Chang Zhou<ericzhou.zc@alibaba-inc.com>.arXiv:2202.03052v2  [cs.CV]  1 Jun 2022', metadata={'source': 'OFA.pdf', 'page': 0}),\n",
       " Document(page_content='1 Introduction\\nBuilding an omnipotent model that handles as many tasks and modalities as human beings is an attractive goal in the AI\\ncommunity. The possibilities of achieving this goal may largely depend on whether massive varieties of modalities,\\ntasks and training regimes can be represented with only a few forms that can be uniﬁed and managed by a single model\\nor system.\\nRecent developments of the Transformer [ 1] architecture have shown its potential for being a universal computation\\nengine [ 2,3,4,5,6,7,8]. In the settings of supervised learning, the “pretrain-ﬁnetune” paradigm achieves excellent\\nsuccess in many domains. In the regimes of few-/zero-shot learning, language models with prompt / instruction tuning\\nprove powerful zero-/few-shot learners [ 3,9,10]. These advances have provided more signiﬁcant than ever opportunities\\nfor the emergence of an omni-model.\\nTo support better generalization for open-ended problems while maintaining multitask performance and ease of use,\\nwe advocate that an omnipotent model should have the following three properties: 1. Task-Agnostic (TA): uniﬁed\\ntask representation to support different types of tasks, including classiﬁcation, generation, self-supervised pretext\\ntasks, etc., and to be agnostic to either pretraining or ﬁnetuning. 2. Modality-Agnostic (MA): uniﬁed input and output\\nrepresentation shared among all tasks to handle different modalities. 3. Task Comprehensiveness (TC): enough task\\nvariety to accumulate generalization ability robustly.\\nHowever, it is challenging to satisfy these properties while maintaining superior performance in downstream tasks.\\nCurrent language and multimodal pretrained models readily fail at parts of these properties, due to their following design\\nchoices. 1. Extra learnable components for ﬁnetuning, e.g., task-speciﬁc heads [ 2], adapters [ 11], soft prompts [ 12].\\nThis makes the model structure task-speciﬁc and poses discrepancy between pretraining and ﬁnetuning. Such designs\\nare also not friendly to supporting unseen tasks in a zero-shot manner. 2. Task-speciﬁc formulation. For most current\\nmethods, pretraining, ﬁnetuning and zero-shot tasks usually differ in task formulation and training objectives. This\\nviolates TA and it is burdensome to scale up the task population to achieve TC. 3. Entangling modality representation\\nwith downstream tasks. It is a common practice for Vision-Language models to take the detected objects as part of\\nthe image input features [8, 13, 14, 15, 16, 17]. Though it demonstrates better downstream task performance on some\\nclosed-domain datasets, it depends on an extra object detector which usually fails at open-domain data.\\nTherefore, we explore an omni-model for multimodal pretraining and propose OFA , hopefully “One For All”, which\\nachieves the objectives of unifying architectures, tasks, and modalities, and supports the three properties above.2\\nWe formulate both pretraining and ﬁnetuning tasks in a uniﬁed sequence-to-sequence abstraction via handcrafted\\ninstructions [ 9,10] to achieve Task-Agnostic. A Transformer is adopted as the Modality-Agnostic compute engine, with\\na constraint that no learnable task- or modality-speciﬁc components will be added to downstream tasks. It is available\\nto represent information from different modalities within a globally shared multimodal vocabulary across all tasks. We\\nthen support Task Comprehensiveness by pretraining on varieties of uni-modal and cross-modal tasks.\\nTo summarize:\\n•We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness.\\nOFA is the ﬁrst attempt to unify the following vision & language, vision-only and language-only tasks,\\nincluding understanding and generation, e.g., text-to-image generation, visual grounding, visual question\\nanswering (VQA), image captioning, image classiﬁcation, language modeling, etc., via a simple sequence-to-\\nsequence learning framework with a uniﬁed instruction-based task representation.\\n•OFA is pretrained on the publicly available datasets of 20M image-text pairs, in comparison with recent\\nmodels that rely on paired data of a much larger scale [ 22,23]. OFA achieves state-of-the-art performances in\\na series of vision & language downstream tasks, including image captioning, visual question answering, visual\\nentailment, referring expression comprehension, etc.\\n•OFA, as a multimodal pretrained model, achieves comparable performances on unimodal tasks with SOTA\\npretrained models in language or vision, e.g., RoBERTa, ELECTRA and DeBERTa for natural language\\nunderstanding, UniLM, Pegasus and ProphetNet for natural language generation, and MoCo-v3, BEiT and\\nMAE for image classiﬁcation.\\n•We verify that OFA achieves competitive performance in zero-shot learning. Also, it can transfer to unseen\\ntasks with new task instructions and adapt to out-of-domain information without ﬁnetuning.\\n2', metadata={'source': 'OFA.pdf', 'page': 1}),\n",
       " Document(page_content='TasksVisual GroundingGrounded CaptioningImage-Text MatchingImage CaptioningVisual Question AnsweringObject DetectionImage InﬁllingText Inﬁlling\\nDetection: What are the objects in the image? ITM: Does the image describe “Two boys playing frisbee on the grass” ? \\nImage Captioning: What does the image describe? \\nVQA: How many people are there in the picture? \\nVG: Which region does the text “Man in white shirt” describe? \\nImage Inﬁlling: What is the image in the middle part?A beautiful woman\\nYes\\nVisualize\\nDecoder\\n…Image vocab.Text vocab.Location vocab.Uniﬁed Vocab.\\n+\\n<loc187><loc47><loc381><loc74>car<loc299><loc126><loc282><loc159>person …\\nText Inﬁlling: What is the complete text of   “A <mask> woman” ? \\nTwo boys playing frisbee on the grass \\n<img123><img756><img311>…<img521> Two\\nVision & Language Tasks+\\nMasking \\nVisualize\\nVision TasksLanguage Tasksdoonpersonis…\\n…<img1><img2><img3><img8192>…\\n…<loc1><loc2><loc3><loc1000>…\\nGC: What does the region describe\"\\x03UHJLRQ\\x1d\\x03<loc299> <loc126> <loc282> <loc159>\\nMan in white shirt  \\n<loc299> <loc126> <loc282> <loc159>OFA\\nFigure 2: A demonstration of the pretraining tasks, including visual grounding, grounded captioning, image-text\\nmatching, image captioning, VQA, object detection, image inﬁlling as well as text inﬁlling.\\n2 Related Work\\nLanguage Pretraining & Vision Pretraining Natural language pretraining has revolutionized the whole NLP\\nresearch community. A representation of this track is the birth of BERT [ 2] and GPT [ 24]. A number of studies have\\nbeen progressively advancing pretraining by improving pretraining tasks and designing more sophisticated model\\narchitectures [ 25,26,27,28,29,30,31]. Having witnessed the success of natural language pretraining, researchers\\nhave promoted self-supervised learning (SSL) in computer vision [ 32,33,34,35]. Recently, mirroring masked language\\nmodeling (MLM) in language pretraining, generative pretraining [ 36,37] with ViT architecture [ 6] further boosts\\ndownstream performance.\\nMultimodal Pretraining Multimodal pretraining has been developing rapidly [ 38,13,39,40,14,41,42,43,44,15,\\n16,17,45,46,47]. Researchers have applied the masking strategies and the encoder-decoder architecture to adapt\\nmodels to generation tasks [ 15,17,18,22]. Besides, to simplify preprocessing, patch projection has received attention\\nand helped Transformer achieve SOTA performance in downstream tasks [ 22,48]. To make full use of large-scale\\nweakly supervised data, [ 49] trains a bi-encoder on 400million pairs and demonstrates excellent performance in retrieval\\ntasks. Another line of work is text-to-image synthesis. A bunch of works [ 50,51,18,52] incorporate Transformer with\\nVQV AE [ 53] or VQGAN [ 54] to generate high-quality images with high resolution. However, the previously mentioned\\nmethods are limited in processing a single type of data, such as cross-modal data only or limited in their capabilities.\\nAlso, the discrepancy between pretraining and ﬁnetuning behaviors limits the transferability to open-ended data.\\nUniﬁed Frameworks To pursue the uniﬁed models, [ 55] demonstrate a uniform format to represent tasks. In NLP,\\nrecent studies unify diverse tasks covering natural language understanding and generation to text-to-text transfer [ 30] or\\nlanguage modeling [ 3]. Following this idea, [ 56] and [ 57] demonstrate text-generation-based multimodal pretrained\\nmodels. [ 7] and [ 58] propose a simple framework that can process information from multiple modalities with a uniform\\nbyte-sequence representation. [ 59] and [ 60] unify tasks of different modalities by designing various task-speciﬁc layers.\\n[61] explores to employ a retrieval-based uniﬁed paradigm. However, these multimodal pretrained models suffer from\\nperformance degradation in downstream tasks, e.g., VQA, image captioning, etc., and they have no image generation\\ncapability.\\n3 OFA\\nIn this work, we propose OFA, a uniﬁed Seq2Seq framework for the uniﬁcation of I/O & architectures, tasks, and\\nmodalities. The overall framework is illustrated in Figure 2.\\n2This work is the latest one of our M6 series [18, 19, 20, 21].\\n3', metadata={'source': 'OFA.pdf', 'page': 2}),\n",
       " Document(page_content='3.1 I/O & Architecture\\nI/O The most common practice of multimodal pretraining is the pretraining of Transformer models on image-text pair\\ncorpus at scale. This requires data preprocessing or modality-speciﬁc adaptors to enable the joint training of both visual\\nand linguistic information with the Transformer architecture. Compared with the complex, resource&time-consuming\\nobject feature extraction, we aim for simplicity and directly use ResNet modules to convolve xv∈RH×W×CtoP\\npatch features of the hidden size, following [ 62] and [ 22]. As to processing the linguistic information, we follow\\nthe practice of GPT [ 24] and BART [ 31] that we apply byte-pair encoding (BPE) [ 63] to the given text sequence to\\ntransform it into a subword sequence and then embed them to features.\\nTo process different modalities without task-speciﬁc output schema, it is essential to represent data of various modalities\\nin a uniﬁed space. A possible solution is to discretize text, image, and object and represent them with tokens in a\\nuniﬁed vocabulary. Recent advances in image quantization [ 53,54] have demonstrated effectiveness in text-to-image\\nsynthesis [ 50,18,51,19], and thus we utilize this strategy for the target-side image representations. Sparse coding is\\neffective in reducing the sequence length of image representation. For example, an image of the resolution of 256×256\\nis represented as a code sequence of the length of 16×16. Each discrete code strongly correlates with the corresponding\\npatch [36].\\nApart from representing images, it is also essential to represent objects within images as there are a series of region-\\nrelated tasks. Following [ 64], we represent objects as a sequence of discrete tokens. To be more speciﬁc, for each\\nobject, we extract its label and its bounding box. The continuous corner coordinates (the top left and the bottom right)\\nof the bounding box are uniformly discretized to integers as location tokens ⟨x1,y1,x2,y2⟩. As to the object labels,\\nthey are intrisically words and thus can be represented with BPE tokens.\\nFinally, we use a uniﬁed vocabulary for all the linguistic and visual tokens, including subwords, image codes, and\\nlocation tokens.\\nArchitecture Following the previous successful practices in multimodal pretraining [ 14,17,22], we choose Trans-\\nformer as the backbone architecture, and we adopt the encoder-decoder framework as the uniﬁed architecture for all the\\npretraining, ﬁnetuning, and zero-shot tasks. Speciﬁcally, both the encoder and the decoder are stacks of Transformer\\nlayers. A Transformer encoder layer consists of a self attention and a feed-forward network (FFN), while a Transformer\\ndecoder layer consists of a self attention, an FFN and a cross attention for building the connection between the decoder\\nand the encoder output representations. To stabilize training and accelerate convergence, we add head scaling to self\\nattention, a post-attention layer normalization (LN) [ 65], and an LN following the ﬁrst layer of FFN [ 66]. For positional\\ninformation, we use two absolute position embeddings for text and images, respectively. Instead of simply adding the\\nposition embeddings, we decoupling the position correlation from token embeddings and patch embeddings [ 67]. In\\naddition, we also use 1D relative position bias for text [30] and 2D relative position bias for image [22, 62].\\n3.2 Tasks & Modalities\\nA uniﬁed framework is designed to provide architecture compatibility across different modalities and downstream tasks\\nso that opportunities can arise to generalize to unseen tasks within the same model. Then we have to represent the\\npossible downstream tasks concerning different modalities in a uniﬁed paradigm. Therefore, an essential point for the\\ndesign of pretraining tasks is the consideration of multitask and multimodality.\\nTo unify tasks and modalities, we design a uniﬁed sequence-to-sequence learning paradigm for pretraining, ﬁnetuning,\\nand inference on all tasks concerning different modalities. Both pretraining tasks and downstream tasks of cross-modal\\nand uni-modal understanding and generation are all formed as Seq2Seq generation. It is available to perform multitask\\npretraining on multimodal and uni-modal data to endow the model with comprehensive capabilities. Speciﬁcally, we\\nshare the identical schema across all tasks, while we specify handcrafted instructions for discrimination [9].\\nFor cross-modal representation learning, we design 5tasks, including visual grounding (VG), grounded captioning\\n(GC), image-text matching (ITM), image captioning (IC), and visual question answering (VQA). For VG, the model\\nlearns to generate location tokens specifying the region position ⟨x1,y1,x2,y2⟩based on the input of the image xiand\\nthe instruction “Which region does the text xtdescribe?” where xtrefers to the region caption. GC is an inverse task\\nof VG. The model learns to generate a description based on the input image xiand the instruction “What does the\\nregion describe? region: ⟨x1,y1,x2,y2⟩”. For ITM, we use each original image-text pair as the positive sample and\\nconstruct a new one as the negative by pairing the image with a randomly substituted caption. The model learns to\\ndiscriminate whether the given image and text are paired by learning to generate “Yes” or “No” based on the input\\nimagexiand the instruction “Does the image describe xt?”. As to image captioning, this task can naturally adapt to the\\nsequence-to-sequence format. The model learns to generate the caption based on the given image and the instruction\\n4', metadata={'source': 'OFA.pdf', 'page': 3}),\n",
       " Document(page_content='Table 1: Detailed hyperparameters of OFA model conﬁguration. We list the conﬁguration for OFA of 5different sizes.\\nModel #Param. Backbone Hidden size Intermediate Size #Head #Enc. Layers #Dec. Layers\\nOFA Tiny 33M ResNet50 256 1024 4 4 4\\nOFA Medium 93M ResNet101 512 2048 8 4 4\\nOFA Base 182M ResNet101 768 3072 12 6 6\\nOFA Large 472M ResNet152 1024 4096 16 12 12\\nOFA Huge 930M ResNet152 1280 5120 16 24 12\\n“What does the image describe?”. For VQA, we send the image and the question as the input and require the model to\\nlearn to generate correct answers.\\nFor uni-modal representation learning, we design 2tasks for vision and 1task for language, respectively. The model is\\npretrained with image inﬁlling and object detection for vision representation learning. Recent advances in generative\\nself-supervised learning for computer vision show that masked image modeling is an effective pretraining task [ 36,37].\\nIn practice, we mask the middle part of the images as the input. The model learns to generate the sparse codes for\\nthe central part of the image based on the corrupted input and the speciﬁed instruction “What is the image in the\\nmiddle part?”. We additionally add object detection to pretraining following [ 44]. The model learns to generate\\nhuman-annotated object representations, i.e., the sequence of object position and label, based on the input image and\\nthe text “What are the objects in the image?” as the instruction. Both tasks strengthen the representation learning on\\nboth pixel and object levels. For language representation learning, following the practice of [ 31], we pretrain the uniﬁed\\nmodel on plain text data with text inﬁlling.\\nIn this way, we unify multiple modalities and multiple tasks to a single model and pretraining paradigm. OFA is\\npretrained jointly with those tasks and data. Thus, it can perform different tasks concerning natural language, vision,\\nand cross-modality.\\n3.3 Pretraining Datasets\\nWe construct pretraining datasets by incorporating Vision & Language data (i.e., image-text pairs), Vision data (i.e., raw\\nimage data, object-labeled data), and Language data (i.e., plain texts). For replication, we only use datasets that are\\npublicly available. We carefully ﬁlter our pretraining data and exclude images that appear in the validation and test sets\\nof downstream tasks to avoid data leakage. We provide more details about pretraining datasets in Appendix A.1.\\n3.4 Training & Inference\\nWe optimize the model with the cross-entropy loss. Given an input x, an instruction sand an output y, we train\\nOFA by minimizing L=−∑|y|\\ni=1logPθ(yi|y<i,x,s), whereθrefers to the model parameters. For inference, we apply\\nthe decoding strategies, e.g., beam search, to enhance the quality of generation. However, this paradigm has several\\nproblems in classiﬁcation tasks: 1. optimizing on the entire vocabulary is unnecessary and inefﬁcient; 2. the model\\nmay generate invalid labels out of the closed label set during inference. To overcome these issues, we introduce a\\nsearch strategy based on preﬁx tree (Trie, [ 68]). Experimental results show that the Trie-based search can enhance the\\nperformance of OFA on classiﬁcation tasks. See Appendix B for more details.\\n3.5 Scaling Models\\nIn order to investigate how OFA of different model sizes perform in downstream tasks, we have developed 5versions of\\nOFA models, scaling from 33M to 940M parameters, and we list their detailed hyperparameters in Table 1.\\nTo be more speciﬁc, we have built basic models of Base andLarge sizes, OFA Base andOFA Large . As our network\\nconﬁguration is similar to BART [ 31], their sizes are similar to those of BART Base andBART Large . Additionally, we\\nhave developed OFA of a larger size, which we name it OFA Huge, or OFA without speciﬁc mentioning in the tables. Its\\nsize is comparable to that of SimVLM Huge orViTHuge. To investigate whether smaller OFA can still reach satisfactory\\nperformance, we have developed OFA Medium andOFA Tiny, which are solely around half and less than 20% as large as\\nOFA Base.\\n5', metadata={'source': 'OFA.pdf', 'page': 4}),\n",
       " Document(page_content='Table 2: Experimental results on cross-modal understanding tasks including VQA and visual entailment. Note that\\nwe report the best results from the previous SOTAs, and speciﬁcally SimVLM is a huge-size model comparable to\\nViT-Huge pretrained on 1.8B image-text pairs, and Florence is built with CoSwin-H and RoBERTa and it is pretrained\\non 900M image-text pairs.\\nModelVQA SNLI-VE\\ntest-dev test-std dev test\\nUNITER [14] 73.8 74.0 79.4 79.4\\nOSCAR [15] 73.6 73.8 - -\\nVILLA [16] 74.7 74.9 80.2 80.0\\nVL-T5 [56] - 70.3 - -\\nVinVL [17] 76.5 76.6 - -\\nUNIMO [46] 75.0 75.3 81.1 80.6\\nALBEF [69] 75.8 76.0 80.8 80.9\\nMETER [70] 77.7 77.6 80.9 81.2\\nVLMo [48] 79.9 80.0 - -\\nSimVLM [22] 80.0 80.3 86.2 86.3\\nFlorence [23] 80.2 80.4 - -\\nOFA Tiny 70.3 70.4 85.3 85.2\\nOFA Medium 75.4 75.5 86.6 87.0\\nOFA Base 78.0 78.1 89.3 89.2\\nOFA Large 80.3 80.5 90.3 90.2\\nOFA 82.0 82.0 91.0 91.2\\nTable 3: Experimental results on MSCOCO Image Captioning. We report the results on the Karpathy test split. Note\\nthat SimVLM and LEMON are huge-size models.\\nModelCross-Entropy Optimization CIDEr Optimization\\nBLEU@4 METEOR CIDEr SPICE BLEU@4 METEOR CIDEr SPICE\\nVL-T5 [56] 34.5 28.7 116.5 21.9 - - - -\\nOSCAR [15] 37.4 30.7 127.8 23.5 41.7 30.6 140.0 24.5\\nUNICORN [57] 35.8 28.4 119.1 21.5 - - - -\\nVinVL [17] 38.5 30.4 130.8 23.4 41.0 31.1 140.9 25.2\\nUNIMO [46] 39.6 - 127.7 - - - - -\\nLEMON [71] 41.5 30.8 139.1 24.1 42.6 31.4 145.5 25.5\\nSimVLM [22] 40.6 33.7 143.3 25.4 - - - -\\nOFA Tiny 35.9 28.1 119.0 21.6 38.1 29.2 128.7 23.1\\nOFA Medium 39.1 30.0 130.4 23.2 41.4 30.8 140.7 24.8\\nOFA Base 41.0 30.9 138.2 24.2 42.8 31.7 146.7 25.8\\nOFA Large 42.4 31.5 142.2 24.5 43.6 32.2 150.7 26.2\\nOFA 43.9 31.8 145.3 24.8 44.9 32.5 154.9 26.6\\n4 Experiments\\nThis section provides experimental details and analyses to demonstrate our model’s effectiveness. See Appendix A for\\nimplementation details.\\n4.1 Results on Cross-modal Tasks\\nWe evaluate our models on different cross-modal downstream tasks, covering cross-modal understanding and generation.\\nSpeciﬁcally, we implement experiments on multimodal understanding datasets including VQAv2 for visual question\\nanswering and SNLI-VE [ 73] for visual entailment, and multimodal generation including MSCOCO Image Caption [ 74]\\nfor image captioning, RefCOCO / RefCOCO+ / RefCOCOg [ 75,76] for referring expression comprehension as this\\n6', metadata={'source': 'OFA.pdf', 'page': 5}),\n",
       " Document(page_content='Table 4: Experimental results on the 3datasets of referring expression comprehension, namely RefCOCO, RefCOCO+,\\nand RefCOCOg. We report the Acc@0.5 on different test splits of the datasets.\\nModelRefCOCO RefCOCO+ RefCOCOg\\nval testA testB val testA testB val-u test-u\\nVL-T5 [56] - - - - - - - 71.3\\nUNITER [14] 81.41 87.04 74.17 75.90 81.45 66.70 74.86 75.77\\nVILLA [16] 82.39 87.48 74.84 76.17 81.54 66.84 76.18 76.71\\nMDETR [72] 86.75 89.58 81.41 79.52 84.09 70.62 81.64 80.89\\nUNICORN [57] 88.29 90.42 83.06 80.30 85.05 71.88 83.44 83.93\\nOFA Tiny 80.20 84.07 75.00 68.22 75.13 57.66 72.02 69.74\\nOFA Medium 85.34 87.68 77.92 76.09 83.04 66.25 78.76 78.58\\nOFA Base 88.48 90.67 83.30 81.39 87.15 74.29 82.29 82.31\\nOFA Large 90.05 92.93 85.26 85.80 89.87 79.22 85.89 86.55\\nOFA 92.04 94.03 88.44 87.86 91.70 80.71 88.07 88.78\\ntask can be viewed as bounding box generation, and MSCOCO Image Caption for text-to-image generation. More\\ndetails are provided in Appendix A.3.\\nTable 2 presents the performance of OFA and baseline models on VQA and SNLI-VE. In general, OFA achieves the\\nbest performance in both tasks with 82.0on the VQA test-std set and 91.2on the SNLI-VE test set. For smaller-size\\nmodels, OFA Large can outperform the recent SOTAs, e.g., VLMo and SimVLM, and OFA Base can beat the SOTAs\\nbefore the aforementioned two models in both tasks. This demonstrates that OFA can achieve superior performance on\\ncross-modal understanding tasks and scaling up OFA can bring signiﬁcant improvements, reﬂecting the strong potential\\nof large-scale pretrained models.\\nTable 3 presents the performance of OFA and baseline models on the MSCOCO image captioning dataset. We report\\nthe results on the Karpathy test split, and we demonstrate the performance of models trained with Cross-Entropy\\noptimization and additionally with CIDEr optimization based on reinforcement learning. In comparison with the\\nprevious SOTA SimVLM Huge for Cross-Entropy optimization, OFA outperforms it by around 2points in CIDEr\\nevaluation. For CIDEr optimization, OFA of the 3sizes all outperform the huge-size LEMON, and OFA demonstrates a\\nnew SOTA of 154.9CIDEr score. By May 31 2022, the single-model OFA had topped the MSCOCO Image Caption\\nLeaderboard.3\\nTo evaluate the capability of visual grounding, we conduct experiments on RefCOCO, RefCOCO+, and RefCOCOg.\\nWhile we unify locations to the vocabulary, visual grounding can be viewed as a sequence generation task. As\\nthere is only one target for each query, we limit the generation length to 4in order to generate a bounding box by\\n<x1,y1,x2,y2>. Experimental results in Table 4 show that OFA reaches the SOTA performance on the 3datasets.\\nCompared with the previous SOTA UNICORN [ 57], OFA achieves signiﬁcant improvement with a gain of 3.61,6.65\\nand4.85points on the testA sets of RefCOCO and RefCOCO+ as well as the test-u set of RefCOCOg.\\nText-to-image generation is a challenging task even for pretrained model. As we pretrain OFA with the task “image-\\ninﬁlling”, i.e., recovering masked patches by generating the corresponding codes [ 36], and thus OFA is able to generate\\ncode. We thus directly ﬁnetune OFA on the MSCOCO Image Caption dataset for text-to-code generation. At the\\ninference stage, we additionally transform the generated codes to an image with the code decoder. Speciﬁcally, we use\\nthe codes from VQGAN [ 54] following [ 52]. Experimental results show that OFA outperforms the baselines in all the\\nmetrics. Note that increasing the sampling size during inference is expected to bring clear improvements on FID and IS.\\nCompared with DALLE [ 50], CogView [ 51] and NÜWA [ 52], whose sampling sizes are 512,60and60, respectively,\\nOFA outperforms these SOTA methods on FID and IS with a much smaller sampling size 24. This illustrates that\\nOFA has learned better correspondence among the query text, the image and the image codes.\\nWe compare OFA with CogView and GLIDE on generation quality with normal and counterfactual queries.4Normal\\nqueries describe existing things in the real world, while counterfactual queries refer to those describing things that could\\nonly exist in our imagination. For normal queries, both CogView and OFA generate images semantically consistent with\\nthe given texts, in comparison with GLIDE. The generated examples from our model can provide more sophisticated\\n3https://competitions.codalab.org/competitions/3221#results\\n4For more implementation details, please refer to Appendix A.3\\n7', metadata={'source': 'OFA.pdf', 'page': 6}),\n",
       " Document(page_content='GLIDEOFACogViewNormal QueryCounterfactual Query\\nA brown horse in the street.A banana in the shape of the bird.Cattle grazing on grass near a lake surrounded by mountain.A street scene with a double-decker bus on the road.An orange clock in the water.A white computer in the sky.\\nFigure 3: Qualitative comparison with state-of-the-art models for text-to-image generation task. We present more\\nqualitative examples of text-to-image generation for better demonstration in Appendix C.\\nTable 5: Experimental results on text-to-image generation. Models are evaluated on FID, CLIPSIM, and IS scores.\\nOFA outperforms the baselines, including the concurrent SOTA NÜWA. We report the results of OFA Large . Note that\\nGLIDE additionally has 1.5Bparameters for upsampling except for the 3.5Bparameters.\\nModel FID ↓CLIPSIM↑ IS↑\\nDALLE [50] 27.5 - 17.9\\nCogView [51] 27.1 33.3 18.2\\nGLIDE [77] 12.2 - -\\nUnifying [78] 29.9 30.9 -\\nNÜWA [52] 12.9 34.3 27.2\\nOFA 10.5 34.4 31.1\\ndetails of objects, say the horse and the double-decker bus. For counterfactual queries, we ﬁnd that OFA is the only one\\nthat can generate the three imaginary scenes, which indicates its imaginative power based on its strong capability to\\nalign text to the image. See Appendix C for more qualitative examples.\\n4.2 Results on Uni-modal Tasks\\nAs the design of OFA uniﬁes different modalities, we evaluate its performance on unimodal tasks, namely tasks\\nof natural language and computer vision. For natural language tasks, we evaluate OFA on 6tasks of the GLUE\\nbenchmark [ 79] for natural language understanding and Gigaword abstractive summarization [ 80] for natural language\\ngeneration. For computer vision, we evaluate OFA on the classic ImageNet-1K [ 81] dataset for image classiﬁcation.\\nMore details are provided in Appendix A.3.\\nAs OFA has been pretrained on plain text data, it can be directly transferred to natural language downstream tasks.\\nFor natural language generation, it is essentially a sequence-to-sequence generation task, and for natural language\\n8', metadata={'source': 'OFA.pdf', 'page': 7}),\n",
       " Document(page_content='Table 6: Experimental results on the GLUE benchmark datasets [ 79]. For comparison, we list the performance of\\nmultimodal pretrained models as well the recent SOTA models that were pretrained on natural language data only.\\nFollowing [28], we ﬁnetune RTE and MRPC starting from the checkpoint ﬁnetuned on MNLI.\\nModel SST-2 RTE MRPC QQP MNLI QNLI\\nMultimodal Pretrained Baseline Models\\nVisualBERT [38] 89.4 56.6 71.9 89.4 81.6 87.0\\nUNITER [14] 89.7 55.6 69.3 89.2 80.9 86.0\\nVL-BERT [8] 89.8 55.7 70.6 89.0 81.2 86.3\\nVilBERT [13] 90.4 53.7 69.0 88.6 79.9 83.8\\nLXMERT [40] 90.2 57.2 69.8 75.3 80.4 84.2\\nUni-Perceiver [61] 90.2 64.3 86.6 87.1 81.7 89.9\\nSimVLM [22] 90.9 63.9 75.2 90.4 83.4 88.6\\nFLA V A [60] 90.9 57.8 81.4 90.4 80.3 87.3\\nUNIMO [46] 96.8 - - - 89.8 -\\nNatural-Language-Pretrained SOTA Models\\nBERT [2] 93.2 70.4 88.0 91.3 86.6 92.3\\nRoBERTa [28] 96.4 86.6 90.9 92.2 90.2 93.9\\nXLNet [25] 97.0 85.9 90.8 92.3 90.8 94.9\\nELECTRA [82] 96.9 88.0 90.8 92.4 90.9 95.0\\nDeBERTa [83] 96.8 88.3 91.9 92.3 91.1 95.3\\nOurs\\nOFA 96.6 91.0 91.7 92.5 90.2 94.8\\nTable 7: Experimental results on Gigaword abstractive summarization. We report performance on the ROUGE\\nevaluation [84].\\nModelGigaword\\nROUGE-1 ROUGE-2 ROUGE-L\\nBERTSHARE [85] 38.13 19.81 35.62\\nMASS [86] 38.73 19.71 35.96\\nUniLM [29] 38.45 19.45 35.75\\nPEGASUS [87] 39.12 19.86 36.24\\nProphetNet [88] 39.55 20.27 36.57\\nUNIMO [46] 39.71 20.37 36.88\\nOFA 39.81 20.66 37.11\\nunderstanding, typically text classiﬁcation, we regard them as generation tasks where labels are essentially word\\nsequences. Additionally, for each task, we design a manual instruction to indicate the model what types of questions it\\nshould answer. We list our instruction design in Appendix A.3.\\nWe demonstrate that even a uniﬁed multimodal pretrained model can achieve highly competitive performance in natural\\nlanguage tasks. Speciﬁcally, in the evaluation of natural language understanding, OFA surpasses multimodal pretrained\\nmodels by large margins in all tasks. In comparison with the state-of-the-art natural language pretrained models,\\nincluding RoBERTa [ 28], XLNET [ 25], ELECTRA [ 82], and DeBERTa [ 83], OFA reaches a comparable performance.\\nIn the evaluation of natural language generation, OFA even reaches a new state-of-the-art performance on the Gigaword\\ndataset.\\nAlso, OFA can reach a competitive performance in image classiﬁcation. Table 8 shows the performance of OFA on\\nimage classiﬁcation. OFA Large achieves higher accuracy than previous backbone models such as EfﬁcientNet-B7 [ 89]\\nand ViT-L [ 6]. We also compare OFA with self-supervised pretraining models based on contrastive learning and masked\\nimage modeling. OFA outperforms contrastive-based models such as SimCLR [ 32] and MoCo-v3 [ 33,35] with similar\\nparameters. Compared with pretrained models based on masked image modeling, e.g., BEiT-L [ 36] and MAE-L [ 37],\\nOFA can achieve similar performance.\\n9', metadata={'source': 'OFA.pdf', 'page': 8}),\n",
       " Document(page_content='Table 8: ImageNet-1K ﬁnetuning results. All the listed models do not use extra labeled image classiﬁcation samples\\nduring training for fair comparison. We report the results of OFA Large .\\nModel Top-1 Acc.\\nEfﬁcientNet-B7 [89] 84.3\\nViT-L/16 [6] 82.5\\nDINO [90] 82.8\\nSimCLR v2 [32] 82.9\\nMoCo v3 [35] 84.1\\nBEiT 384-L/16 [36] 86.3\\nMAE-L/16 [37] 85.9\\nOFA 85.6\\nTable 9: Zero-shot performance on 6GLUE tasks and SNLI-VE.\\nModelSST-2 RTE MRPC QQP QNLI MNLI SNLI-VE\\nAcc. Acc. F1 F1 Acc. Acc. Acc. (dev/test)\\nUni-Perceiver 70.6 55.6 76.1 53.6 51.0 49.6 -\\nOFA Base 71.6 56.7 79.5 54.0 51.4 37.3 49.71 / 49.18\\nThese aforementioned results in both natural language and vision tasks indicate that a uniﬁed multimodal pretrained\\nmodel is not only effective in multimodal tasks but also capable of tackling unimodal tasks, and in the future, it might\\nbe sufﬁcient for such a model to solve complex tasks concerning different modality combinations.\\n4.3 Zero-shot Learning & Task Transfer\\nThe instruction-guided pretraining enables OFA to perform zero-shot inference. Following Uni-Perceiver [ 61], we\\nevaluate our model on the 6tasks of the GLUE benchmark, including single-sentence classiﬁcation and sentence pair\\nclassiﬁcation. Table 9 demonstrates that OFA generally outperforms Uni-Perceiver. However, both models do not\\nachieve satisfactory performance in sentence-pair classiﬁcation (with Acc.<60%). We hypothesize that the missing\\nsentence-pair data in the pretraining dataset attributes to the performance.\\nAlso, we ﬁnd that the model performance is highly sensitive to the design of instructions. To obtain the best result,\\none should search a proper instruction template possibly from a large pool of candidates. A slight change to manual\\nprompts or model parameters may drastically inﬂuence the model performance, which is not robust. We leave this issue\\nto the future work.\\nWe observe that the model can transfer to unseen tasks well with new task instructions. We design a new task called\\ngrounded question answering and present examples in Figure 4. In this scenario, given a question about a certain region\\non the image, the model should provide a correct answer. We ﬁnd that the model can achieve a satisfactory performance\\nin this new task, which reﬂects its strong transferability. Besides, OFA can solve tasks with the out-of-domain input\\ndata. For example, OFA without ﬁnetuning achieves satisfactory performance in VQA for the out-of-domain images.\\nExamples are demonstrated in Figure 5. OFA can also perform accurate visual grounding on the out-of-domain images,\\ne.g., anime pictures, synthetic images, etc., and we demonstrate more examples on Figure 11 in Appendix C.\\n4.4 Ablation on Multitask Pretraining\\nThanks to the uniﬁed framework, OFA has been pretrained on multiple tasks and thus endowed with comprehensive\\ncapabilities. However, the effects of each task are still undiscovered. We verify their effects on multiple downstream\\ntasks, including image captioning, VQA, image classiﬁcation, and text-to-image generation.\\nWe ﬁrst evaluate how uni-modal pretraining tasks inﬂuence the performance in both cross-modal and uni-modal\\ntasks. Table 10 demonstrates our experimental results. We observe some interesting phenomena about the effects of\\nuni-modal pretraining tasks. Text inﬁlling brings improvement on image caption ( +0.8CIDEr) and VQA ( +0.46Acc.).\\nNatural language pretraining betters the contextualized representation of language and thus enhances performance in\\ncross-modal tasks. However, it is noticed that the language pretraining task may degrade the performance in image\\n10', metadata={'source': 'OFA.pdf', 'page': 9}),\n",
       " Document(page_content='what color is the car in the region? region: <loc301> <loc495> <loc501> <loc596>what color is the car in the region? region: <loc512> <loc483> <loc675> <loc576>tangrayQ:A:A:Q:Figure 4: Qualitative results on an unseen task grounded QA. We design a new task called grounded question answering,\\nwhere the model should answer a question about a certain region in the image. More samples are provided in Figure 10\\nin Appendix C.\\nwhat is grown on the plant?moneyQ:A:what does the red-roofed building right to the big airship look like?D\\x03PXVKURRPA:Q:\\nFigure 5: Qualitative results on unseen domain VQA. During pretraining, only real-world photographs are used for\\nVQA. We present cases of VQA on out-of-domain images, i.e., the iconic and sci-ﬁ images, and demonstrate their\\ncapability of transferring to unseen domains. More samples are provided in Figure 9 in Appendix C.\\nclassiﬁcation, leading to the decrease in ImageNet-1K ( −1.0Acc.). Also, it is interesting to ﬁnd that it does not\\nencourage improvement in text-to-image generation ( −0.1CLIPSIM). It may attribute to the simplicity of text in\\nthis task, which indicates that improved representation of language does not affect the performance. As to image\\ninﬁlling, it signiﬁcantly improves the performance in image classiﬁcation ( +1.0Acc.) and text-to-image generation\\n(+0.6CLIPSIM). Learning to recover images is an effective self-supervised task for image representation, and it also\\nencourages the decoder’s ability to generate image codes. However, it hurts the performance in image captioning and\\nVQA. Both tasks require a strong capability in generating texts, and the decoder’s learning of image generation naturally\\nbrings performance degradation in captioning ( −0.7CIDEr) and VQA ( −0.3Acc.).\\nFurthermore, we evaluate how multimodal tasks impact the performance. Previous studies have provided evidence\\nof the contribution of conventional pretraining tasks, e.g., MLM, MOC, ITM, VQA, image captioning, etc. [ 14,17].\\nHowever, they miss other tasks, e.g., detection and visual grounding & grounded captioning. We conduct experiments\\non these tasks and ﬁnd that tasks predicting regions are crucial to multimodal tasks, with a performance increase in\\nimage captioning ( +2.3CIDEr & +1.4CIDEr) and VQA ( +0.6Acc. & +0.5Acc.). It suggests that detection and\\nvisual grounding & grounded captioning help the model grasp ﬁned-grained alignments between vision and language.\\n11', metadata={'source': 'OFA.pdf', 'page': 10}),\n",
       " Document(page_content='Table 10: Ablation results of OFA. All models are pretrained for 250ksteps. w/o ground. represents the removal of\\nboth visual grounding and grounded captioning tasks. Note that all models are only ﬁnetuned with the cross-entropy\\nloss in image captioning.\\nModelCaption VQA ImageNet Image Generation\\nCIDEr Test-dev Top-1 Acc. FID / CLIPSIM / IS\\nOFA Base 135.6 76.0 82.2 20.8 / 31.6 / 21.5\\nw/o text inﬁll. 134.8 75.6 83.2 20.3 / 31.7 / 21.8\\nw/o image inﬁll. 136.3 76.3 81.8 23.2 / 31.0 / 20.0\\nw/o det. 133.3 75.4 81.4 20.9 / 31.5 / 21.6\\nw/o ground. 134.2 75.5 82.0 21.2 / 31.5 / 21.5\\nRegion information contributes little to text-to-image generation ( +0.1CLIPSIM & +0.1CLIPSIM), as this task\\nrequires far less text-region alignment information. We surprisingly ﬁnd that detection can encourage the performance\\nin visual understanding ( +0.8Acc.). It indicates that incorporating region information might be essential to visual\\nunderstanding, especially on images with complex objects.\\n5 Conclusion\\nIn this work, we propose OFA , a Task-Agnostic and Modality-Agnostic framework supporting Task Comprehensiveness.\\nOFA achieves the uniﬁcation in architecture, tasks and modalities, and thus is capable of multimodal & uni-modal\\nunderstanding and generation, without speciﬁcation in additional layers or tasks. Our experiments show that OFA creates\\nnew SOTAs in a series of tasks, including image captioning, VQA, visual entailment, and referring expression\\ncomprehension. OFA also demonstrates a comparable performance with language / vision pretrained SOTA models in\\nuni-modal understanding and generation tasks, e.g., GLUE, abstractive summarization, and image classiﬁcation. We\\nprovide a further analysis to demonstrate its capability in zero-shot learning and domain & task transfer, and we also\\nverify the effectiveness of pretraining tasks.\\nIn the future, we will continue exploring the issues discovered in this work. Also, we endeavor to ﬁgure out a reasonable\\nsolution to building an omni-model essentially generalizable to the complex real world.\\nAcknowledgments\\nWe would like to thank Jie Zhang, Yong Li, Jiamang Wang, Shao Yuan, and Zheng Cao for their support to this project,\\nand we would like to thank Guangxiang Zhao and Fei Sun for their insightful comments to our paper.\\nReferences\\n[1]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need. In NeurIPS 2017, pages 5998–6008, 2017.\\n[2]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirec-\\ntional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors,\\nNAACL-HLT 2019, pages 4171–4186. Association for Computational Linguistics, 2019.\\n[3]Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv\\npreprint arXiv:2005.14165, 2020.\\n[4]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and\\nJohn Schulman. Training veriﬁers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n[5]Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised pre-training for\\nspeech recognition. arXiv preprint arXiv:1904.05862, 2019.\\n[6]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:\\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n12', metadata={'source': 'OFA.pdf', 'page': 11}),\n",
       " Document(page_content='[7]Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver:\\nGeneral perception with iterative attention. arXiv preprint arXiv:2103.03206, 2021.\\n[8]Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic\\nvisual-linguistic representations. In International Conference onLearning Representations, 2019.\\n[9]Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai,\\nand Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\\n[10] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaf-\\nﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task\\ngeneralization. arXiv preprint arXiv:2110.08207, 2021.\\n[11] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo,\\nMona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp. In International Conference on\\nMachine Learning, pages 2790–2799. PMLR, 2019.\\n[12] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning.\\narXiv preprint arXiv:2104.08691, 2021.\\n[13] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic represen-\\ntations for vision-and-language tasks. In NeurIPS, 2019.\\n[14] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\\nUniter: Universal image-text representation learning. In ECCV, 2020.\\n[15] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong,\\nFuru Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks.\\nInECCV, 2020.\\n[16] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for\\nvision-and-language representation learning. ArXiv, abs/2006.06195, 2020.\\n[17] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.\\nVinvl: Revisiting visual representations in vision-language models. 2021 IEEE/CVF Conference onComputer\\nVision andPattern Recognition (CVPR), pages 5575–5584, 2021.\\n[18] Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang,\\nXianyan Jia, et al. M6: A chinese multimodal pretrainer. arXiv preprint arXiv:2103.00823, 2021.\\n[19] Zhu Zhang, Jianxin Ma, Chang Zhou, Rui Men, Zhikang Li, Ming Ding, Jie Tang, Jingren Zhou, and Hongxia\\nYang. M6-ufc: Unifying multi-modal controls for conditional image synthesis. arXiv preprint arXiv:2105.14211 ,\\n2021.\\n[20] An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang,\\nYong Li, et al. Exploring sparse expert models and beyond. arXiv preprint arXiv:2105.15082, 2021.\\n[21] Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin,\\net al. M6-10t: A sharing-delinking paradigm for efﬁcient multi-trillion parameter pretraining. arXiv preprint\\narXiv:2110.03888, 2021.\\n[22] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual\\nlanguage model pretraining with weak supervision. ArXiv, abs/2108.10904, 2021.\\n[23] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel C. F. Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong\\nHuang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng\\nWang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and Pengchuan Zhang. Florence: A\\nnew foundation model for computer vision. ArXiv, abs/2111.11432, 2021.\\n[24] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\\nstanding by generative pre-training. URL https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/\\nlanguageunsupervised/language understanding paper. pdf, 2018.\\n[25] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. Xlnet:\\nGeneralized autoregressive pretraining for language understanding. In NeurIPS 2019, pages 5754–5764, 2019.\\n[26] Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian,\\nand Hua Wu. ERNIE: enhanced representation through knowledge integration. CoRR, abs/1904.09223, 2019.\\n[27] Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE 2.0: A\\ncontinual pre-training framework for language understanding. CoRR, abs/1907.12412, 2019.\\n13', metadata={'source': 'OFA.pdf', 'page': 12}),\n",
       " Document(page_content='[28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR ,\\nabs/1907.11692, 2019.\\n[29] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-\\nWuen Hon. Uniﬁed language model pre-training for natural language understanding and generation. In NeurIPS\\n2019, pages 13042–13054, 2019.\\n[30] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\\nLi, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of\\nMachine Learning Research, 21(140):1–67, 2020.\\n[31] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin\\nStoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language\\ngeneration, translation, and comprehension. In ACL 2020, July 2020.\\n[32] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive\\nlearning of visual representations. In International conference onmachine learning , pages 1597–1607. PMLR,\\n2020.\\n[33] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive\\nlearning. arXiv preprint arXiv:2003.04297, 2020.\\n[34] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl\\nDoersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own\\nlatent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.\\n[35] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings ofthe\\nIEEE/CVF Conference onComputer Vision andPattern Recognition, pages 15750–15758, 2021.\\n[36] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\\narXiv:2106.08254, 2021.\\n[37] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are\\nscalable vision learners. arXiv preprint arXiv:2111.06377, 2021.\\n[38] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and\\nperformant baseline for vision and language. ArXiv, abs/1908.03557, 2019.\\n[39] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao. Uniﬁed vision-\\nlanguage pre-training for image captioning and VQA. In AAAI 2020, pages 13041–13049, 2020.\\n[40] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers.\\nInProceedings ofthe2019 Conference onEmpirical Methods inNatural Language Processing andthe9th\\nInternational Joint Conference onNatural Language Processing (EMNLP-IJCNLP), pages 5100–5111, 2019.\\n[41] Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou. Unicoder-vl: A universal encoder for vision and\\nlanguage by cross-modal pre-training. CoRR, abs/1908.06066, 2019.\\n[42] Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, and Hongxia Yang. Interbert: Vision-and-language\\ninteraction for multi-modal pretraining. arXiv preprint arXiv:2003.13198, 2020.\\n[43] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-task vision and\\nlanguage representation learning. In Proceedings oftheIEEE/CVF Conference onComputer Vision andPattern\\nRecognition, pages 10437–10446, 2020.\\n[44] Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, and Fei Huang. E2e-vlp:\\nEnd-to-end vision-language pre-training enhanced by visual learning. arXiv preprint arXiv:2106.01804, 2021.\\n[45] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil: Knowledge enhanced\\nvision-language representations through scene graphs. In Proceedings oftheAAAI Conference onArtiﬁcial\\nIntelligence, volume 35, pages 3208–3216, 2021.\\n[46] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. UNIMO:\\ntowards uniﬁed-modal understanding and generation via cross-modal contrastive learning. In Chengqing\\nZong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, ACL/IJCNLP 2021, pages 2592–2607. Association for\\nComputational Linguistics, 2021.\\n[47] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels\\nwith text by deep multi-modal transformers. ArXiv, abs/2004.00849, 2020.\\n[48] Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. Vlmo: Uniﬁed vision-language pre-training with mixture-\\nof-modality-experts. ArXiv, abs/2111.02358, 2021.\\n14', metadata={'source': 'OFA.pdf', 'page': 13}),\n",
       " Document(page_content='[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual\\nmodels from natural language supervision. In Marina Meila and Tong Zhang, editors, ICML 2021 , volume 139\\nofProceedings ofMachine Learning Research, pages 8748–8763. PMLR, 2021.\\n[50] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya\\nSutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\\n[51] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\\nShao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. arXiv preprint\\narXiv:2105.13290, 2021.\\n[52] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N \\\\\" uwa: Visual synthesis\\npre-training for neural visual world creation. arXiv preprint arXiv:2111.12417, 2021.\\n[53] Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NIPS ,\\n2017.\\n[54] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In\\nProceedings oftheIEEE/CVF Conference onComputer Vision andPattern Recognition , pages 12873–12883,\\n2021.\\n[55] Lukasz Kaiser, Aidan N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, and Jakob Uszkoreit.\\nOne model to learn them all. arXiv preprint arXiv:1706.05137, 2017.\\n[56] Jaemin Cho, Jie Lei, Haochen Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation.\\nInICML, 2021.\\n[57] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan\\nWang. Crossing the format boundary of text and boxes: Towards uniﬁed vision-language modeling. ArXiv ,\\nabs/2111.12085, 2021.\\n[58] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda\\nKoppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture for structured\\ninputs & outputs. arXiv preprint arXiv:2107.14795, 2021.\\n[59] Ronghang Hu and Amanpreet Singh. Unit: Multimodal multitask learning with a uniﬁed transformer. arXiv\\npreprint arXiv:2102.10772, 2021.\\n[60] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach,\\nand Douwe Kiela. Flava: A foundational language and vision alignment model. arXiv preprint arXiv:2112.04482 ,\\n2021.\\n[61] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Xiaogang Wang, Hongsheng Li, Xiaohua Wang, and Jifeng Dai.\\nUni-perceiver: Pre-training uniﬁed architecture for generic perception for zero-shot and few-shot tasks. arXiv\\npreprint arXiv:2112.01522, 2021.\\n[62] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all\\ndata sizes. arXiv preprint arXiv:2106.04803, 2021.\\n[63] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword\\nunits. In Proceedings ofthe54th Annual Meeting oftheAssociation forComputational Linguistics (V olume 1:\\nLong Papers), pages 1715–1725, 2016.\\n[64] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language modeling\\nframework for object detection. arXiv preprint arXiv:2109.10852, 2021.\\n[65] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR , abs/1607.06450, 2016.\\n[66] Sam Shleifer, Jason Weston, and Myle Ott. Normformer: Improved transformer pretraining with extra normaliza-\\ntion. arXiv preprint arXiv:2110.09456, 2021.\\n[67] Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In International\\nConference onLearning Representations, 2020.\\n[68] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. Introduction toalgorithms . MIT\\npress, 2009.\\n[69] Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shaﬁq Joty, Caiming Xiong, and Steven Hoi.\\nAlign before fuse: Vision and language representation learning with momentum distillation. In Thirty-Fifth\\nConference onNeural Information Processing Systems, 2021.\\n15', metadata={'source': 'OFA.pdf', 'page': 14}),\n",
       " Document(page_content='[70] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Nanyun Peng,\\nZicheng Liu, and Michael Zeng. An empirical study of training end-to-end vision-and-language transformers.\\nArXiv, abs/2111.02387, 2021.\\n[71] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up\\nvision-language pre-training for image captioning. CoRR, abs/2111.12233, 2021.\\n[72] Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan Misra, Gabriel Synnaeve, and Nicolas Carion. Mdetr -\\nmodulated detection for end-to-end multi-modal understanding. ArXiv, abs/2104.12763, 2021.\\n[73] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: A novel task for ﬁne-grained image\\nunderstanding. arXiv preprint arXiv:1901.06706, 2019.\\n[74] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence\\nZitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 , 2015.\\n[75] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring\\nexpressions. In European Conference onComputer Vision, pages 69–85. Springer, 2016.\\n[76] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation\\nand comprehension of unambiguous object descriptions. In Proceedings oftheIEEE conference oncomputer\\nvision andpattern recognition, pages 11–20, 2016.\\n[77] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever,\\nand Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models.\\narXiv preprint arXiv:2112.10741, 2021.\\n[78] Yupan Huang, Hongwei Xue, Bei Liu, and Yutong Lu. Unifying multimodal transformer for bi-directional\\nimage and text generation. In Proceedings ofthe29th ACM International Conference onMultimedia , pages\\n1138–1147, 2021.\\n[79] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-\\ntask benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461 ,\\n2018.\\n[80] Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence\\nsummarization. In Proceedings ofthe2015 Conference onEmpirical Methods inNatural Language Processing ,\\npages 379–389, 2015.\\n[81] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\\nimage database. In 2009 IEEE conference oncomputer vision andpattern recognition , pages 248–255. Ieee,\\n2009.\\n[82] Kevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. ELECTRA: pre-training text\\nencoders as discriminators rather than generators. In 8thInternational Conference onLearning Representations,\\nICLR 2020. OpenReview.net, 2020.\\n[83] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: decoding-enhanced bert with disen-\\ntangled attention. In 9thInternational Conference onLearning Representations, ICLR 2021 . OpenReview.net,\\n2021.\\n[84] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches\\nOut, Barcelona, Spain, July 2004. Association for Computational Linguistics.\\n[85] Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. Leveraging pre-trained checkpoints for sequence generation\\ntasks. Transactions oftheAssociation forComputational Linguistics, 8:264–280, 2020.\\n[86] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: masked sequence to sequence pre-training\\nfor language generation. In ICML 2019, pages 5926–5936, 2019.\\n[87] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. Pegasus: Pre-training with extracted gap-sentences\\nfor abstractive summarization. In International Conference onMachine Learning , pages 11328–11339. PMLR,\\n2020.\\n[88] Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou.\\nProphetnet: Predicting future n-gram for sequence-to-sequence pre-training. In Proceedings ofthe2020\\nConference onEmpirical Methods inNatural Language Processing: Findings, pages 2401–2410, 2020.\\n[89] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. In\\nInternational Conference onMachine Learning, pages 6105–6114. PMLR, 2019.\\n[90] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.\\nEmerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294, 2021.\\n16', metadata={'source': 'OFA.pdf', 'page': 15}),\n",
       " Document(page_content='[91] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale\\nimage-text pre-training to recognize long-tail visual concepts. In Proceedings oftheIEEE/CVF Conference on\\nComputer Vision andPattern Recognition, pages 3558–3568, 2021.\\n[92] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed,\\nimage alt-text dataset for automatic image captioning. In ACL 2018, pages 2556–2565, 2018.\\n[93] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned\\nphotographs. In NeurIPS 2011, pages 1143–1151, 2011.\\n[94] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis\\nKalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting\\nlanguage and vision using crowdsourced dense image annotations. International Journal ofComputer Vision ,\\n123(1):32–73, 2017.\\n[95] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter:\\nElevating the role of image understanding in visual question answering. In Proceedings oftheIEEE Conference\\nonComputer Vision andPattern Recognition, pages 6904–6913, 2017.\\n[96] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and composi-\\ntional question answering. In CVPR 2019, pages 6700–6709, 2019.\\n[97] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth,\\nand Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications oftheACM , 59(2):64–73,\\n2016.\\n[98] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali,\\nStefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4. International Journal\\nofComputer Vision, 128(7):1956–1981, 2020.\\n[99] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Ob-\\njects365: A large-scale, high-quality dataset for object detection. In Proceedings oftheIEEE/CVF International\\nConference onComputer Vision, pages 8430–8439, 2019.\\n[100] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,\\nAnish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv\\npreprint arXiv:2101.00027, 2020.\\n[101] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\\nCVPR 2016, pages 770–778, 2016.\\n[102] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR 2019, 2019.\\n[103] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth.\\nInECCV, 2016.\\n[104] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of\\nmachine translation. In Proceedings ofthe40th annual meeting oftheAssociation forComputational Linguistics ,\\npages 311–318, 2002.\\n[105] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation\\nwith human judgments. In Proceedings oftheaclworkshop onintrinsic andextrinsic evaluation measures for\\nmachine translation and/or summarization, pages 65–72, 2005.\\n[106] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\\nevaluation. In Proceedings oftheIEEE conference oncomputer vision andpattern recognition , pages 4566–\\n4575, 2015.\\n[107] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image\\ncaption evaluation. In European conference oncomputer vision, pages 382–398. Springer, 2016.\\n[108] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In\\nProceedings oftheIEEE conference oncomputer vision andpattern recognition, pages 3128–3137, 2015.\\n[109] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by\\na two time-scale update rule converge to a local nash equilibrium. Advances inneural information processing\\nsystems, 30, 2017.\\n[110] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved\\ntechniques for training gans. Advances inneural information processing systems, 29:2234–2242, 2016.\\n17', metadata={'source': 'OFA.pdf', 'page': 16}),\n",
       " Document(page_content='[111] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence\\ntraining for image captioning. In Proceedings oftheIEEE conference oncomputer vision andpattern recognition ,\\npages 7008–7024, 2017.\\n[112] Guangxiang Zhao, Wenkai Yang, Xuancheng Ren, Lei Li, and Xu Sun. Well-classiﬁed examples are underesti-\\nmated in classiﬁcation with deep neural networks. CoRR, abs/2110.06537, 2021.\\n[113] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data\\naugmentation with a reduced search space. In Proceedings oftheIEEE/CVF Conference onComputer Vision\\nandPattern Recognition Workshops, pages 702–703, 2020.\\n[114] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In\\nProceedings oftheAAAI Conference onArtiﬁcial Intelligence, volume 34, pages 13001–13008, 2020.\\n[115] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk min-\\nimization. In 6thInternational Conference onLearning Representations, ICLR 2018, Vancouver, BC,Canada,\\nApril 30-May 3,2018, Conference Track Proceedings. OpenReview.net, 2018.\\n[116] Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe. Cutmix:\\nRegularization strategy to train strong classiﬁers with localizable features. In 2019 IEEE/CVF International\\nConference onComputer Vision, ICCV 2019, Seoul, Korea (South), October 27-November 2,2019 , pages\\n6022–6031. IEEE, 2019.\\n[117] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta,\\nTheo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-ﬁltered 400 million\\nimage-text pairs. arXiv preprint arXiv:2111.02114, 2021.\\n18', metadata={'source': 'OFA.pdf', 'page': 17}),\n",
       " Document(page_content='A Implementation Details\\nA.1 Pretraining Datasets\\nWe construct pretraining datasets by incorporating Vision & Language data (i.e., image-text pairs), Vision data (i.e.,\\nraw image data, object-labeled data), and Language data (i.e., plain texts). For replication, the pretraining datasets are\\npublicly available. We carefully ﬁlter our pretraining data and exclude images that appear in the validation and test sets\\nof downstream tasks to avoid data leakage. The statistics on the pretraining datasets are listed in Table 11.\\nCross-modal Data For vision & language pretraining, we mainly apply image-text pairs, including image-caption\\npairs, image-QA pairs, and image-region pairs, as the pretraining data. For the pretraining tasks of image captioning\\nand image-text matching, we collect Conceptual Caption 12M (CC12M) [ 91], Conceptual Captions (CC3M) [ 92],\\nSBU [ 93], MSCOCO image captions (COCO) [ 74], and Visual Genome Captions (VG Captions) [ 94]. Speciﬁcally,\\nthe part of data from VG requires some additional processing. As texts in VG captions describe local regions on\\nthe images, we retrieve regions with area larger than 16,384pixels and construct region-caption pairs. For visual\\nquestion answering, we collect VQAv2 [ 95], VG-QA [ 94], as well as GQA [ 96]. VQAv2 is a visual question answering\\ndataset with real-world photographs from COCO. VG-QA is also a visual question answering dataset with real-world\\nphotographs from VG. The questions of VG-QA are related to speciﬁc regions on the images. GQA is a large VQA\\ndataset featuring compositional questions. The images of GQA are also collected from VG. For visual grounding\\nand grounded captioning, we collect data from RefCOCO [ 75], RefCOCO+ [ 75], RefCOCOg [ 76] and VG captions.\\nAdditional processing is applied to VG Captions for this task. Speciﬁcally, we use the data of VG that contains regions\\nwith area smaller than 16,384pixels for Visual Grounding, in order to encourage model to grasp ﬁne-grained alignments\\nbetween vision and language.\\nUni-modal Data Uni-modal data includes vision and language data. Vision data consists of raw images for image\\ninﬁlling and object-labeled images for object detection. For image inﬁlling, we collect raw images from OpenImages,\\nYFCC100M [ 97] and ImageNet-21K [ 81], and exclude annotations. Thus the model is unable to access labels in\\nthe pretraining stage. For object detection, we collect OpenImages [ 98], Object365 [ 99], VG and COCO for object\\ndetection. Language data consists of plain texts, i.e., passages consisting of sentences. We use around 140GB of data\\nfrom Pile [ 100] to leverage its diversity. Speciﬁcally, we extract natural language data and implement preprocessing\\nmethods, including truncation to the length of 512.\\nTable 11: Statistics on the datasets of pretraining tasks. “#Image” denotes the number of distinct images, and “#Sample”\\ndenotes the number of samples. *For language data, we report its storage following the previous studies [2, 28].\\nType Pretraining Task Source #Image #Sample\\nVision & LanguageImage CaptioningCC12M, CC3M, SBU, COCO, VG-Cap 14.78M 15.25MImage-Text Matching\\nVisual Question Answering VQAv2, VG-QA, GQA 178K 2.92M\\nVisual GroundingRefCOCO, RefCOCO+, RefCOCOg, VG-Cap 131K 3.20MGrounded Captioning\\nVisionDetection OpenImages, Object365, VG, COCO 2.98M 3.00M\\nImage Inﬁlling OpenImages, YFCC100M, ImageNet-21K 36.27M -\\nLanguage Masked Language Modeling Pile (Filtered) - 140GB*\\nA.2 Pretraining Details\\nFor the image processing, we ﬁrst resize and crop the images into different resolutions, 256×256forOFA Tiny and\\nOFA Medium ,384×384forOFA Base,480×480forOFA Large andOFA Huge, with a ﬁxed patch size of 16×16. Note\\nthat training OFA Large andOFA Huge are time and computation consuming, we ﬁrst train them with images of the\\nresolution of 384×384and256×256, and continue pretraining with images of the resolution of 480×480.\\nFor each patch, we obtain its feature vector with the ﬁrst three blocks of ResNet [ 101]. The ResNet module is jointly\\ntrained along with the transformer module. Note that through extensive experiments we ﬁnd that random sampling\\npatches [ 47] does not bring additional beneﬁts in our scenario. For the text processing, we tokenize the texts with the\\n19', metadata={'source': 'OFA.pdf', 'page': 18}),\n",
       " Document(page_content='same BPE Tokenizer [ 63] as BART [ 31]. The maximum text sequence length of both encoder and decoder is set to 256.\\nWe share parameters between the embedding and the decoder softmax output layer.\\nFrom our preliminary experiments, we ﬁnd that the initialization for Transformer plays an important role. For OFA Base\\nandOFA Large , we initialize the transformer with most of the weights of BART Base andBART Large considering the\\nslight difference between OFA Transformer and BART as described in Sec 3.1. For OFA of the other sizes, we\\npretrain language models with the same pretraining strategy with BART and use the pretrained weights to initialize the\\nTransformer in OFA.\\nWe use the AdamW [ 102] optimizer with (β1,β2) = (0.9,0.999) andϵ= 1e-8to pretrain our models. We set the\\npeak learning rate to 2e-4, and apply a scheduler with linear decay with a warmup ratio of 0.01to control the learning\\nrate. For regulation, we set dropout to 0.1and use weight decay with 0.01. We employ stochastic depth [ 103] with a\\n0.1rate (applied to encoder and decoder except for convolution blocks). We mix all the pretraining data within each\\nbatch, which contains 2,048vision&language samples, 256object detection samples, 256image-only samples and 512\\ntext-only samples. All models are pretrained for at least 300Ksteps except the models used for ablation study.\\nA.3 Details of Downstream Tasks\\nWe verify the capability of OFA on various downstream tasks in both ﬁnetuning and zero-shot settings. We design\\nvarious task-speciﬁc instructions to transfer the knowledge learned from pretraining to downstream tasks effectively.\\nThe instructions of different tasks are listed in Table 12. For ﬁnetuning, if not speciﬁed, the input image resolution is set\\nto480×480, and the other hyper-parameters remain the same as for pretraining. The experimental details of different\\ndownstream tasks, including both multimodal and uni-modal tasks, are listed below:\\nImage Captioning Image captioning is a standard vision&language task that requires models to generate an ap-\\npropriate and ﬂuent caption for an image. We adopt the most widely used MSCOCO Image Caption dataset [ 74]\\nto evaluate the multi-modal generation capability of OFA. We report BLEU-4 [ 104], METEOR [ 105], CIDEr [ 106],\\nand SPICE [ 107] scores on the Karpathy test split [ 108]. Following the previous standard practice, we ﬁrst ﬁnetune\\nOFA with cross-entropy loss for 2epochs with a batch size of 128and a learning rate of 1e−5, and label smoothing\\nis set to 0.1. We then ﬁnetune the model with CIDEr optimization for 3epochs with a batch size of 64, and disable\\ndropout and stochastic depth. We report both scores at the two stages.\\nVisual Question Answering Visual question answering (VQA) is a cross-modal task that requires the models to\\nanswer the question given an image. Previous works such as VLMo [ 48] or SimVLM [ 22] deﬁne VQA as a classiﬁcation\\ntask. They use a linear output layer to predict the probability of each candidate answer on a given set. In contrast with\\nthese studies, to adapt the generative OFA model to VQA benchmark, we use the Trie-based search strategy mentioned\\nin Sec. 3.4 to ensure that the answer generated by OFA is constrained in the candidate set. We evaluate our model\\nwith other baselines on the commonly used VQAv2 dataset [ 95]. Accuracy scores on both test-dev and test-std sets\\nare reported. The OFA models of all the reported sizes are ﬁnetuned for 40,000steps with a batch size of 512. The\\nlearning rate is 5e−5with the label smoothing of 0.1. When ﬁnetuning OFA Large andOFA Huge, we increase the\\nimage resolution from 480to640. Linear interpolation of the image absolute positional embedding proposed in [ 6] is\\nemployed when transferring the pretrained OFA to VQA ﬁnetuning. During Trie-based searching, we constrain the\\ngenerated answers over the most frequent 3,129answer candidates. Exponential moving average (EMA) with decay\\nrate0.9999 is employed in ﬁnetuning.\\nVisual Entailment Visual entailment requires the model to evaluate how the given image and text are semantically\\ncorrelated, i.e., entailment, neutral, or contradiction. We perform experiments on the SNLI-VE dataset [ 73]. The image\\npremise, text premise and text hypothesis are fed to the encoder, and the decoder generates appropriate labels. To\\ntransfer the knowledge learned by pretraining to this task, we convert the labels entailment/neutral/contradiction to\\nyes/maybe/no. We also use the Trie-based search strategy to constrain the generated labels over the candidate set. We\\nreport accuracy on both dev and test sets. The OFA model is ﬁnetuned for 6epochs with a learning rate of 2e−5and a\\nbatch size of 256.\\nReferring Expression Comprehension Referring expression comprehension requires models to locate an image\\nregion described by a language query. Different from the approach taken by most previous methods [ 13,14] which\\nranks a set of candidate bounding boxes detected by a pretrained object detector, our method directly predicts the best\\nmatching bounding box without any proposals. We perform experiments on RefCOCO [ 75], RefCOCO+ [ 75], and\\nRefCOCOg [ 76]. Consistent with other downstream tasks, we formulate referring expression comprehension as a\\nconditional sequence generation task. In detail, given an image and a language query, OFA generates the box sequence\\n(e.g.,⟨x1,y1,x2,y2⟩) in an autoregressive manner. We report the standard metric Acc@0.5 on the validation and test\\n20', metadata={'source': 'OFA.pdf', 'page': 19}),\n",
       " Document(page_content='Table 12: Instructions for downstream tasks.\\nTask Dataset Instruction Target\\nImage Captioning COCO [Image ]What does the image describe? { Caption }\\nVisual Question\\nAnsweringVQA [Image ]{Question } { Answer }\\nVisual Entailment SNLI-VE [Image ]Can image and text1 “{ Text1 }\" imply text2 “{ Text2 }\"? Yes/No/Maybe\\nReferring Expression\\nComprehensionRefCOCO,\\nRefCOCO+,\\nRefCOCOg[Image ]Which region does the text “{ Text}\" describe? {Location }\\nImage Generation COCO What is the complete image? caption: { Caption } { Image }\\nImage Classiﬁcation ImageNet-1K [Image ]What does the image describe? { Label }\\nSingle-Sentence\\nClassiﬁcationSST-2 Is the sentiment of text “{ Text}\" positive or negative? Positive/Negative\\nSentence-Pair\\nClassiﬁcationRTE Can text1 “{ Text1 }\" imply text2 “{ Text2 }\"? Yes/No\\nMRPC Does text1 “{ Text1 }\" and text2 “{ Text2 }\" have the same semantics? Yes/No\\nQQP Is question “{ Question1 }\" and question “{ Question2 }\" equivalent? Yes/No\\nMNLI Can text1 “{ Text1 }\" imply text2 “{ Text2 }\"? Yes/No/Maybe\\nQNLI Does “{ Text}\" contain the answer to question “{ Question }\"? Yes/No\\nText Summarization Gigaword What is the summary of article “{ Article }\"? { Summary }\\nsets. For ﬁnetuning, the input image resolution is set to 512×512. We ﬁnetune the OFA model on each dataset for\\nabout 10epochs with a batch size of 128. The learning rate is 3e−5with the label smoothing of 0.1. Each query only\\ncorresponds to an image region, so we limit the maximum generated length to 4during inference.\\nImage Generation Following the same setting with [ 52], we train our model on the MS COCO train split and evaluate\\nour model on the validation split by randomly sampling 30,000images. We use Fréchet Inception Distance (FID) [ 109]\\nand Inception Score (IS) [ 110] to evaluate the quality of the images. Following the previous studies [ 78,52], we also\\ncompute CLIP Similarity Score (CLIPSIM) to evaluate the semantic similarity between the query text and the generated\\nimages. During ﬁnetuning, OFA learns to generate the image code sequence according to the given text query only.\\nThe model is ﬁrst ﬁnetuned with cross-entropy and then with CLIPSIM optimization following [ 111,78]. In the ﬁrst\\nstage, we ﬁnetune the OFA model for about 50epochs with a batch size of 512and a learning rate of 1e−3. In the\\nsecond stage, the model is ﬁnetuned for extra 5000 steps with a batch size of 32and a learning rate of 1e−6. During\\nthe evaluation, we sample 24images with the resolution of 256×256for each query and choose the best one using the\\npretrained CLIP model [49].\\nFor case study, we compare OFA with CogView and GLIDE. CogView provides an API website5. Note that this API\\nsamples 8 images of resolution of 512×512for each query. We select the ﬁrst one of generated images and resize it to\\nthe resolution of 256×256. GLIDE provides a Colab notebook.6. Note that the only publicly available GLIDE model\\nis of base size (∼385M).\\nImage Classiﬁcation We provide ﬁnetuning results on ImageNet-1K [ 81] following recent studies in self-supervised\\nlearning for computer vision. During ﬁnetuning and inference, a Trie-based search strategy is employed to constrain\\nthe generated text into the set of 1,000 candidate labels. We ﬁnetune OFA for 32epochs and a batch size of\\n256. The learning rate is 5e−5. The ratio for label smoothing is 0.1. The encouraging loss proposed in [ 112] is\\nemployed with the hyperparameter LE set to 0.75. Following [ 36], we use the same random resize cropping, random\\nﬂipping, RandAug [113] and random erasing [114] transformations as data augmentation strategies. Mixup [115] and\\nCutMix [ 116] are used with overall 0.5probability to be performed on each batch and alpha is 0.8and1.0, respectively.\\nTo adapt the mixed soft target of Mixup and CutMix into generation paradigm during ﬁnetuning, we run the decoder\\ntwice each with one of the target sequences to be mixed and sum the loss weighted by the mixing ratio.\\nNatural Language Understanding To verify the natural language understanding ability of OFA, we select 6language\\nunderstanding tasks from GLUE benchmark [ 79], including both single-sentence classiﬁcation tasks and sentence-pair\\n5https://wudao.aminer.cn/CogView/index.html\\n6https://colab.research.google.com/drive/1q6tJ58UKod1eCOkbaUNGzF3K5BbXlB5m\\n21', metadata={'source': 'OFA.pdf', 'page': 20}),\n",
       " Document(page_content='classiﬁcation tasks. To adapt to sentence-pair classiﬁcation, previous models [ 2,28] usually use segment embeddings to\\ndistinguish different sentences. Unlike those models, OFA can apply the model to sentence-pair classiﬁcation tasks by\\nconstructing appropriate instructions without introducing additional segment embeddings. For the hyper-parameters of\\nﬁnetuning, we tune the training epochs among {5,7,10}, learning rate among {3e−5,5e−5,6e−5,7e−5,1e−4},\\nbatch size among{32,64,128}, weight decay among {0.01,0.05}, and dropout rate among {0.0,0.1}. We report the\\nbest performance on the development set for each task.\\nNatural Language Generation We verify the natural language generation ability of OFA in the Gigaword dataset [ 80].\\nWe report ROUGE-1/ROUGE-2/ROUGE-L to evaluate the generation results following [ 80]. We ﬁnetune the OFA mod-\\nels for 6epochs with a batch size of 512. The learning rate is 1e−4with the label smoothing of 0.1, and the maximum\\ninput text sequence length is set to 512. During inference, we set the length penalty to 0.7and beam size to 6, and limit\\nthe maximum generated length to 32.\\nB Trie-based Search\\nThis section describes how to use Trie-based search to improve model performance on downstream classiﬁcation\\ntasks. When dealing with classiﬁcation tasks, we ﬁrst construct a Trie where nodes are annotated with tokens from the\\ncandidate label-set. During ﬁnetuning, the model computes the log-probabilities of the target tokens based on their\\npositions on the Trie. As shown in Figure 6, when computing the log-probabilities of the target token “sky”, we only\\nconsider tokens in {“sky”, “ocean”} and forcefully set the logits for all invalid tokens to −∞. During inference, we\\nconstrain the generated labels over the candidate set. As shown in Table 13, Trie-based search strategy can boost the\\nperformance of OFA in various downstream classiﬁcation tasks.\\nFigure 6: Example of Trie-based search where the constraint labels are “blue sky”, “blue ocean” and “green”. When\\ncomputing the log-prob of token “sky”, we only consider tokens in {“sky”, “ocean”} and forcefully set the logits for all\\ninvalid tokens to−∞.\\nTable 13: Ablation results of Trie. The removal of Trie-based search degenerates the performance on downstream tasks.\\nNote that the baseline OFA Base is only pre-trained for 250k steps, which is also used in Table 10.\\nModelVQA SNLI-VE ImageNet MRPC QQP\\nTest-dev Acc. Dev Acc. Top-1 Acc. F1 F1\\nOFA Base 76.03 89.2 82.2 90.6 88.4\\nw/o Trie 75.86(-0.17) 89.0(-0.2) 81.9(-0.3) 90.1(-0.5) 88.2(-0.2)\\nC Qualitative Examples\\nThis section provides more qualitative examples of multiple tasks, including text-to-image generation, open-domain\\nVQA, grounded question answering, and open-domain visual grounding, from the generation of OFA. By reading this\\nsection, we hope that readers can better perceive OFA.\\n22', metadata={'source': 'OFA.pdf', 'page': 21}),\n",
       " Document(page_content='Figure 7: Examples of text-to-image generation. For better demonstration, we continue ﬁnetuning OFA on a subset of\\nLAION-400M [117].\\n23', metadata={'source': 'OFA.pdf', 'page': 22}),\n",
       " Document(page_content='Figure 8: Examples of text-to-image generation.\\n24', metadata={'source': 'OFA.pdf', 'page': 23}),\n",
       " Document(page_content='Figure 9: More samples of VQA task on unseen domains. The answers are generated by pretrained OFA without\\nﬁnetuning. The datasets used in VQA pretraining task only contain real-world photographs. We present more cases of\\nVQA task on out-of-domain (non-photographic) images and demonstrate the capability of transferring OFA to these\\nunseen domains.\\nFigure 10: Samples of the unseen grounded question answering task. In this task, the model should answer a question\\nabout a particular region in the image. This task is unseen in pretraining. We demonstrate that directly transferring\\npretrained OFA to this new task without ﬁnetuning works well.\\n25', metadata={'source': 'OFA.pdf', 'page': 24}),\n",
       " Document(page_content='Figure 11: Samples of visual grounding task generated by OFA for various unseen domains: (a) anime (the corresponding\\nanimations are Pokemon andOne Piece ); (b) synthetic images with attribute combinations.\\n26', metadata={'source': 'OFA.pdf', 'page': 25})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader(\"OFA.pdf\")\n",
    "text_document_3 = pdf_loader.load()\n",
    "text_document_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='OFA: U NIFYING ARCHITECTURES , TASKS ,AND MODALITIES\\nTHROUGH A SIMPLE SEQUENCE -TO-SEQUENCE LEARNING\\nFRAMEWORK\\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai\\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang\\nDAMO Academy, Alibaba Group∗\\n{zheluo.wp, ya235025, menrui.mr, junyang.ljy, baishuai.bs,\\nzhikang.lzk, jason.mjx, ericzhou.zc, jingren.zhou, yang.yhx}@alibaba-inc.com\\nFigure 1: Examples of various tasks supported by OFA.\\nABSTRACT\\nIn this work, we pursue a uniﬁed paradigm for multimodal pretraining to break the scaffolds of\\ncomplex task/modality-speciﬁc customization. We propose OFA, a Task-Agnostic and Modality-\\nAgnostic framework that supports Task Comprehensiveness. OFA uniﬁes a diverse set of cross-\\nmodal and unimodal tasks, including image generation, visual grounding, image captioning, image\\nclassiﬁcation, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA', metadata={'source': 'OFA.pdf', 'page': 0}),\n",
       " Document(page_content='modal and unimodal tasks, including image generation, visual grounding, image captioning, image\\nclassiﬁcation, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA\\nfollows the instruction-based learning in both pretraining and ﬁnetuning stages, requiring no extra\\ntask-speciﬁc layers for downstream tasks. In comparison with the recent state-of-the-art vision\\n& language models that rely on extremely large cross-modal datasets, OFA is pretrained on only\\n20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training\\ndata, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive\\nperformances on uni-modal tasks. Our further analysis indicates that OFA can also effectively\\ntransfer to unseen tasks and unseen domains. Our code and models are publicly available at https:\\n//github.com/OFA-Sys/OFA .\\nKeywords Uniﬁed frameworks ·Multimodal pretraining ·Multitask learning ·Zero-shot learning', metadata={'source': 'OFA.pdf', 'page': 0}),\n",
       " Document(page_content='//github.com/OFA-Sys/OFA .\\nKeywords Uniﬁed frameworks ·Multimodal pretraining ·Multitask learning ·Zero-shot learning\\n∗Correspondence to: Chang Zhou<ericzhou.zc@alibaba-inc.com>.arXiv:2202.03052v2  [cs.CV]  1 Jun 2022', metadata={'source': 'OFA.pdf', 'page': 0}),\n",
       " Document(page_content='1 Introduction\\nBuilding an omnipotent model that handles as many tasks and modalities as human beings is an attractive goal in the AI\\ncommunity. The possibilities of achieving this goal may largely depend on whether massive varieties of modalities,\\ntasks and training regimes can be represented with only a few forms that can be uniﬁed and managed by a single model\\nor system.\\nRecent developments of the Transformer [ 1] architecture have shown its potential for being a universal computation\\nengine [ 2,3,4,5,6,7,8]. In the settings of supervised learning, the “pretrain-ﬁnetune” paradigm achieves excellent\\nsuccess in many domains. In the regimes of few-/zero-shot learning, language models with prompt / instruction tuning\\nprove powerful zero-/few-shot learners [ 3,9,10]. These advances have provided more signiﬁcant than ever opportunities\\nfor the emergence of an omni-model.\\nTo support better generalization for open-ended problems while maintaining multitask performance and ease of use,', metadata={'source': 'OFA.pdf', 'page': 1}),\n",
       " Document(page_content='for the emergence of an omni-model.\\nTo support better generalization for open-ended problems while maintaining multitask performance and ease of use,\\nwe advocate that an omnipotent model should have the following three properties: 1. Task-Agnostic (TA): uniﬁed\\ntask representation to support different types of tasks, including classiﬁcation, generation, self-supervised pretext\\ntasks, etc., and to be agnostic to either pretraining or ﬁnetuning. 2. Modality-Agnostic (MA): uniﬁed input and output\\nrepresentation shared among all tasks to handle different modalities. 3. Task Comprehensiveness (TC): enough task\\nvariety to accumulate generalization ability robustly.\\nHowever, it is challenging to satisfy these properties while maintaining superior performance in downstream tasks.\\nCurrent language and multimodal pretrained models readily fail at parts of these properties, due to their following design', metadata={'source': 'OFA.pdf', 'page': 1}),\n",
       " Document(page_content='Current language and multimodal pretrained models readily fail at parts of these properties, due to their following design\\nchoices. 1. Extra learnable components for ﬁnetuning, e.g., task-speciﬁc heads [ 2], adapters [ 11], soft prompts [ 12].\\nThis makes the model structure task-speciﬁc and poses discrepancy between pretraining and ﬁnetuning. Such designs\\nare also not friendly to supporting unseen tasks in a zero-shot manner. 2. Task-speciﬁc formulation. For most current\\nmethods, pretraining, ﬁnetuning and zero-shot tasks usually differ in task formulation and training objectives. This\\nviolates TA and it is burdensome to scale up the task population to achieve TC. 3. Entangling modality representation\\nwith downstream tasks. It is a common practice for Vision-Language models to take the detected objects as part of\\nthe image input features [8, 13, 14, 15, 16, 17]. Though it demonstrates better downstream task performance on some', metadata={'source': 'OFA.pdf', 'page': 1}),\n",
       " Document(page_content='the image input features [8, 13, 14, 15, 16, 17]. Though it demonstrates better downstream task performance on some\\nclosed-domain datasets, it depends on an extra object detector which usually fails at open-domain data.\\nTherefore, we explore an omni-model for multimodal pretraining and propose OFA , hopefully “One For All”, which\\nachieves the objectives of unifying architectures, tasks, and modalities, and supports the three properties above.2\\nWe formulate both pretraining and ﬁnetuning tasks in a uniﬁed sequence-to-sequence abstraction via handcrafted\\ninstructions [ 9,10] to achieve Task-Agnostic. A Transformer is adopted as the Modality-Agnostic compute engine, with\\na constraint that no learnable task- or modality-speciﬁc components will be added to downstream tasks. It is available\\nto represent information from different modalities within a globally shared multimodal vocabulary across all tasks. We', metadata={'source': 'OFA.pdf', 'page': 1}),\n",
       " Document(page_content='to represent information from different modalities within a globally shared multimodal vocabulary across all tasks. We\\nthen support Task Comprehensiveness by pretraining on varieties of uni-modal and cross-modal tasks.\\nTo summarize:\\n•We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness.\\nOFA is the ﬁrst attempt to unify the following vision & language, vision-only and language-only tasks,\\nincluding understanding and generation, e.g., text-to-image generation, visual grounding, visual question\\nanswering (VQA), image captioning, image classiﬁcation, language modeling, etc., via a simple sequence-to-\\nsequence learning framework with a uniﬁed instruction-based task representation.\\n•OFA is pretrained on the publicly available datasets of 20M image-text pairs, in comparison with recent\\nmodels that rely on paired data of a much larger scale [ 22,23]. OFA achieves state-of-the-art performances in', metadata={'source': 'OFA.pdf', 'page': 1}),\n",
       " Document(page_content='models that rely on paired data of a much larger scale [ 22,23]. OFA achieves state-of-the-art performances in\\na series of vision & language downstream tasks, including image captioning, visual question answering, visual\\nentailment, referring expression comprehension, etc.\\n•OFA, as a multimodal pretrained model, achieves comparable performances on unimodal tasks with SOTA\\npretrained models in language or vision, e.g., RoBERTa, ELECTRA and DeBERTa for natural language\\nunderstanding, UniLM, Pegasus and ProphetNet for natural language generation, and MoCo-v3, BEiT and\\nMAE for image classiﬁcation.\\n•We verify that OFA achieves competitive performance in zero-shot learning. Also, it can transfer to unseen\\ntasks with new task instructions and adapt to out-of-domain information without ﬁnetuning.\\n2', metadata={'source': 'OFA.pdf', 'page': 1}),\n",
       " Document(page_content='TasksVisual GroundingGrounded CaptioningImage-Text MatchingImage CaptioningVisual Question AnsweringObject DetectionImage InﬁllingText Inﬁlling\\nDetection: What are the objects in the image? ITM: Does the image describe “Two boys playing frisbee on the grass” ? \\nImage Captioning: What does the image describe? \\nVQA: How many people are there in the picture? \\nVG: Which region does the text “Man in white shirt” describe? \\nImage Inﬁlling: What is the image in the middle part?A beautiful woman\\nYes\\nVisualize\\nDecoder\\n…Image vocab.Text vocab.Location vocab.Uniﬁed Vocab.\\n+\\n<loc187><loc47><loc381><loc74>car<loc299><loc126><loc282><loc159>person …\\nText Inﬁlling: What is the complete text of   “A <mask> woman” ? \\nTwo boys playing frisbee on the grass \\n<img123><img756><img311>…<img521> Two\\nVision & Language Tasks+\\nMasking \\nVisualize\\nVision TasksLanguage Tasksdoonpersonis…\\n…<img1><img2><img3><img8192>…\\n…<loc1><loc2><loc3><loc1000>…', metadata={'source': 'OFA.pdf', 'page': 2}),\n",
       " Document(page_content='<img123><img756><img311>…<img521> Two\\nVision & Language Tasks+\\nMasking \\nVisualize\\nVision TasksLanguage Tasksdoonpersonis…\\n…<img1><img2><img3><img8192>…\\n…<loc1><loc2><loc3><loc1000>…\\nGC: What does the region describe\"\\x03UHJLRQ\\x1d\\x03<loc299> <loc126> <loc282> <loc159>\\nMan in white shirt  \\n<loc299> <loc126> <loc282> <loc159>OFA\\nFigure 2: A demonstration of the pretraining tasks, including visual grounding, grounded captioning, image-text\\nmatching, image captioning, VQA, object detection, image inﬁlling as well as text inﬁlling.\\n2 Related Work\\nLanguage Pretraining & Vision Pretraining Natural language pretraining has revolutionized the whole NLP\\nresearch community. A representation of this track is the birth of BERT [ 2] and GPT [ 24]. A number of studies have\\nbeen progressively advancing pretraining by improving pretraining tasks and designing more sophisticated model\\narchitectures [ 25,26,27,28,29,30,31]. Having witnessed the success of natural language pretraining, researchers', metadata={'source': 'OFA.pdf', 'page': 2}),\n",
       " Document(page_content='architectures [ 25,26,27,28,29,30,31]. Having witnessed the success of natural language pretraining, researchers\\nhave promoted self-supervised learning (SSL) in computer vision [ 32,33,34,35]. Recently, mirroring masked language\\nmodeling (MLM) in language pretraining, generative pretraining [ 36,37] with ViT architecture [ 6] further boosts\\ndownstream performance.\\nMultimodal Pretraining Multimodal pretraining has been developing rapidly [ 38,13,39,40,14,41,42,43,44,15,\\n16,17,45,46,47]. Researchers have applied the masking strategies and the encoder-decoder architecture to adapt\\nmodels to generation tasks [ 15,17,18,22]. Besides, to simplify preprocessing, patch projection has received attention\\nand helped Transformer achieve SOTA performance in downstream tasks [ 22,48]. To make full use of large-scale\\nweakly supervised data, [ 49] trains a bi-encoder on 400million pairs and demonstrates excellent performance in retrieval', metadata={'source': 'OFA.pdf', 'page': 2}),\n",
       " Document(page_content='weakly supervised data, [ 49] trains a bi-encoder on 400million pairs and demonstrates excellent performance in retrieval\\ntasks. Another line of work is text-to-image synthesis. A bunch of works [ 50,51,18,52] incorporate Transformer with\\nVQV AE [ 53] or VQGAN [ 54] to generate high-quality images with high resolution. However, the previously mentioned\\nmethods are limited in processing a single type of data, such as cross-modal data only or limited in their capabilities.\\nAlso, the discrepancy between pretraining and ﬁnetuning behaviors limits the transferability to open-ended data.\\nUniﬁed Frameworks To pursue the uniﬁed models, [ 55] demonstrate a uniform format to represent tasks. In NLP,\\nrecent studies unify diverse tasks covering natural language understanding and generation to text-to-text transfer [ 30] or\\nlanguage modeling [ 3]. Following this idea, [ 56] and [ 57] demonstrate text-generation-based multimodal pretrained', metadata={'source': 'OFA.pdf', 'page': 2}),\n",
       " Document(page_content='language modeling [ 3]. Following this idea, [ 56] and [ 57] demonstrate text-generation-based multimodal pretrained\\nmodels. [ 7] and [ 58] propose a simple framework that can process information from multiple modalities with a uniform\\nbyte-sequence representation. [ 59] and [ 60] unify tasks of different modalities by designing various task-speciﬁc layers.\\n[61] explores to employ a retrieval-based uniﬁed paradigm. However, these multimodal pretrained models suffer from\\nperformance degradation in downstream tasks, e.g., VQA, image captioning, etc., and they have no image generation\\ncapability.\\n3 OFA\\nIn this work, we propose OFA, a uniﬁed Seq2Seq framework for the uniﬁcation of I/O & architectures, tasks, and\\nmodalities. The overall framework is illustrated in Figure 2.\\n2This work is the latest one of our M6 series [18, 19, 20, 21].\\n3', metadata={'source': 'OFA.pdf', 'page': 2}),\n",
       " Document(page_content='3.1 I/O & Architecture\\nI/O The most common practice of multimodal pretraining is the pretraining of Transformer models on image-text pair\\ncorpus at scale. This requires data preprocessing or modality-speciﬁc adaptors to enable the joint training of both visual\\nand linguistic information with the Transformer architecture. Compared with the complex, resource&time-consuming\\nobject feature extraction, we aim for simplicity and directly use ResNet modules to convolve xv∈RH×W×CtoP\\npatch features of the hidden size, following [ 62] and [ 22]. As to processing the linguistic information, we follow\\nthe practice of GPT [ 24] and BART [ 31] that we apply byte-pair encoding (BPE) [ 63] to the given text sequence to\\ntransform it into a subword sequence and then embed them to features.\\nTo process different modalities without task-speciﬁc output schema, it is essential to represent data of various modalities', metadata={'source': 'OFA.pdf', 'page': 3}),\n",
       " Document(page_content='transform it into a subword sequence and then embed them to features.\\nTo process different modalities without task-speciﬁc output schema, it is essential to represent data of various modalities\\nin a uniﬁed space. A possible solution is to discretize text, image, and object and represent them with tokens in a\\nuniﬁed vocabulary. Recent advances in image quantization [ 53,54] have demonstrated effectiveness in text-to-image\\nsynthesis [ 50,18,51,19], and thus we utilize this strategy for the target-side image representations. Sparse coding is\\neffective in reducing the sequence length of image representation. For example, an image of the resolution of 256×256\\nis represented as a code sequence of the length of 16×16. Each discrete code strongly correlates with the corresponding\\npatch [36].\\nApart from representing images, it is also essential to represent objects within images as there are a series of region-', metadata={'source': 'OFA.pdf', 'page': 3}),\n",
       " Document(page_content='patch [36].\\nApart from representing images, it is also essential to represent objects within images as there are a series of region-\\nrelated tasks. Following [ 64], we represent objects as a sequence of discrete tokens. To be more speciﬁc, for each\\nobject, we extract its label and its bounding box. The continuous corner coordinates (the top left and the bottom right)\\nof the bounding box are uniformly discretized to integers as location tokens ⟨x1,y1,x2,y2⟩. As to the object labels,\\nthey are intrisically words and thus can be represented with BPE tokens.\\nFinally, we use a uniﬁed vocabulary for all the linguistic and visual tokens, including subwords, image codes, and\\nlocation tokens.\\nArchitecture Following the previous successful practices in multimodal pretraining [ 14,17,22], we choose Trans-\\nformer as the backbone architecture, and we adopt the encoder-decoder framework as the uniﬁed architecture for all the', metadata={'source': 'OFA.pdf', 'page': 3}),\n",
       " Document(page_content='former as the backbone architecture, and we adopt the encoder-decoder framework as the uniﬁed architecture for all the\\npretraining, ﬁnetuning, and zero-shot tasks. Speciﬁcally, both the encoder and the decoder are stacks of Transformer\\nlayers. A Transformer encoder layer consists of a self attention and a feed-forward network (FFN), while a Transformer\\ndecoder layer consists of a self attention, an FFN and a cross attention for building the connection between the decoder\\nand the encoder output representations. To stabilize training and accelerate convergence, we add head scaling to self\\nattention, a post-attention layer normalization (LN) [ 65], and an LN following the ﬁrst layer of FFN [ 66]. For positional\\ninformation, we use two absolute position embeddings for text and images, respectively. Instead of simply adding the\\nposition embeddings, we decoupling the position correlation from token embeddings and patch embeddings [ 67]. In', metadata={'source': 'OFA.pdf', 'page': 3}),\n",
       " Document(page_content='position embeddings, we decoupling the position correlation from token embeddings and patch embeddings [ 67]. In\\naddition, we also use 1D relative position bias for text [30] and 2D relative position bias for image [22, 62].\\n3.2 Tasks & Modalities\\nA uniﬁed framework is designed to provide architecture compatibility across different modalities and downstream tasks\\nso that opportunities can arise to generalize to unseen tasks within the same model. Then we have to represent the\\npossible downstream tasks concerning different modalities in a uniﬁed paradigm. Therefore, an essential point for the\\ndesign of pretraining tasks is the consideration of multitask and multimodality.\\nTo unify tasks and modalities, we design a uniﬁed sequence-to-sequence learning paradigm for pretraining, ﬁnetuning,\\nand inference on all tasks concerning different modalities. Both pretraining tasks and downstream tasks of cross-modal', metadata={'source': 'OFA.pdf', 'page': 3}),\n",
       " Document(page_content='and inference on all tasks concerning different modalities. Both pretraining tasks and downstream tasks of cross-modal\\nand uni-modal understanding and generation are all formed as Seq2Seq generation. It is available to perform multitask\\npretraining on multimodal and uni-modal data to endow the model with comprehensive capabilities. Speciﬁcally, we\\nshare the identical schema across all tasks, while we specify handcrafted instructions for discrimination [9].\\nFor cross-modal representation learning, we design 5tasks, including visual grounding (VG), grounded captioning\\n(GC), image-text matching (ITM), image captioning (IC), and visual question answering (VQA). For VG, the model\\nlearns to generate location tokens specifying the region position ⟨x1,y1,x2,y2⟩based on the input of the image xiand\\nthe instruction “Which region does the text xtdescribe?” where xtrefers to the region caption. GC is an inverse task', metadata={'source': 'OFA.pdf', 'page': 3}),\n",
       " Document(page_content='the instruction “Which region does the text xtdescribe?” where xtrefers to the region caption. GC is an inverse task\\nof VG. The model learns to generate a description based on the input image xiand the instruction “What does the\\nregion describe? region: ⟨x1,y1,x2,y2⟩”. For ITM, we use each original image-text pair as the positive sample and\\nconstruct a new one as the negative by pairing the image with a randomly substituted caption. The model learns to\\ndiscriminate whether the given image and text are paired by learning to generate “Yes” or “No” based on the input\\nimagexiand the instruction “Does the image describe xt?”. As to image captioning, this task can naturally adapt to the\\nsequence-to-sequence format. The model learns to generate the caption based on the given image and the instruction\\n4', metadata={'source': 'OFA.pdf', 'page': 3}),\n",
       " Document(page_content='Table 1: Detailed hyperparameters of OFA model conﬁguration. We list the conﬁguration for OFA of 5different sizes.\\nModel #Param. Backbone Hidden size Intermediate Size #Head #Enc. Layers #Dec. Layers\\nOFA Tiny 33M ResNet50 256 1024 4 4 4\\nOFA Medium 93M ResNet101 512 2048 8 4 4\\nOFA Base 182M ResNet101 768 3072 12 6 6\\nOFA Large 472M ResNet152 1024 4096 16 12 12\\nOFA Huge 930M ResNet152 1280 5120 16 24 12\\n“What does the image describe?”. For VQA, we send the image and the question as the input and require the model to\\nlearn to generate correct answers.\\nFor uni-modal representation learning, we design 2tasks for vision and 1task for language, respectively. The model is\\npretrained with image inﬁlling and object detection for vision representation learning. Recent advances in generative\\nself-supervised learning for computer vision show that masked image modeling is an effective pretraining task [ 36,37].', metadata={'source': 'OFA.pdf', 'page': 4}),\n",
       " Document(page_content='self-supervised learning for computer vision show that masked image modeling is an effective pretraining task [ 36,37].\\nIn practice, we mask the middle part of the images as the input. The model learns to generate the sparse codes for\\nthe central part of the image based on the corrupted input and the speciﬁed instruction “What is the image in the\\nmiddle part?”. We additionally add object detection to pretraining following [ 44]. The model learns to generate\\nhuman-annotated object representations, i.e., the sequence of object position and label, based on the input image and\\nthe text “What are the objects in the image?” as the instruction. Both tasks strengthen the representation learning on\\nboth pixel and object levels. For language representation learning, following the practice of [ 31], we pretrain the uniﬁed\\nmodel on plain text data with text inﬁlling.\\nIn this way, we unify multiple modalities and multiple tasks to a single model and pretraining paradigm. OFA is', metadata={'source': 'OFA.pdf', 'page': 4}),\n",
       " Document(page_content='model on plain text data with text inﬁlling.\\nIn this way, we unify multiple modalities and multiple tasks to a single model and pretraining paradigm. OFA is\\npretrained jointly with those tasks and data. Thus, it can perform different tasks concerning natural language, vision,\\nand cross-modality.\\n3.3 Pretraining Datasets\\nWe construct pretraining datasets by incorporating Vision & Language data (i.e., image-text pairs), Vision data (i.e., raw\\nimage data, object-labeled data), and Language data (i.e., plain texts). For replication, we only use datasets that are\\npublicly available. We carefully ﬁlter our pretraining data and exclude images that appear in the validation and test sets\\nof downstream tasks to avoid data leakage. We provide more details about pretraining datasets in Appendix A.1.\\n3.4 Training & Inference\\nWe optimize the model with the cross-entropy loss. Given an input x, an instruction sand an output y, we train\\nOFA by minimizing L=−∑|y|', metadata={'source': 'OFA.pdf', 'page': 4}),\n",
       " Document(page_content='3.4 Training & Inference\\nWe optimize the model with the cross-entropy loss. Given an input x, an instruction sand an output y, we train\\nOFA by minimizing L=−∑|y|\\ni=1logPθ(yi|y<i,x,s), whereθrefers to the model parameters. For inference, we apply\\nthe decoding strategies, e.g., beam search, to enhance the quality of generation. However, this paradigm has several\\nproblems in classiﬁcation tasks: 1. optimizing on the entire vocabulary is unnecessary and inefﬁcient; 2. the model\\nmay generate invalid labels out of the closed label set during inference. To overcome these issues, we introduce a\\nsearch strategy based on preﬁx tree (Trie, [ 68]). Experimental results show that the Trie-based search can enhance the\\nperformance of OFA on classiﬁcation tasks. See Appendix B for more details.\\n3.5 Scaling Models\\nIn order to investigate how OFA of different model sizes perform in downstream tasks, we have developed 5versions of', metadata={'source': 'OFA.pdf', 'page': 4}),\n",
       " Document(page_content='3.5 Scaling Models\\nIn order to investigate how OFA of different model sizes perform in downstream tasks, we have developed 5versions of\\nOFA models, scaling from 33M to 940M parameters, and we list their detailed hyperparameters in Table 1.\\nTo be more speciﬁc, we have built basic models of Base andLarge sizes, OFA Base andOFA Large . As our network\\nconﬁguration is similar to BART [ 31], their sizes are similar to those of BART Base andBART Large . Additionally, we\\nhave developed OFA of a larger size, which we name it OFA Huge, or OFA without speciﬁc mentioning in the tables. Its\\nsize is comparable to that of SimVLM Huge orViTHuge. To investigate whether smaller OFA can still reach satisfactory\\nperformance, we have developed OFA Medium andOFA Tiny, which are solely around half and less than 20% as large as\\nOFA Base.\\n5', metadata={'source': 'OFA.pdf', 'page': 4}),\n",
       " Document(page_content='Table 2: Experimental results on cross-modal understanding tasks including VQA and visual entailment. Note that\\nwe report the best results from the previous SOTAs, and speciﬁcally SimVLM is a huge-size model comparable to\\nViT-Huge pretrained on 1.8B image-text pairs, and Florence is built with CoSwin-H and RoBERTa and it is pretrained\\non 900M image-text pairs.\\nModelVQA SNLI-VE\\ntest-dev test-std dev test\\nUNITER [14] 73.8 74.0 79.4 79.4\\nOSCAR [15] 73.6 73.8 - -\\nVILLA [16] 74.7 74.9 80.2 80.0\\nVL-T5 [56] - 70.3 - -\\nVinVL [17] 76.5 76.6 - -\\nUNIMO [46] 75.0 75.3 81.1 80.6\\nALBEF [69] 75.8 76.0 80.8 80.9\\nMETER [70] 77.7 77.6 80.9 81.2\\nVLMo [48] 79.9 80.0 - -\\nSimVLM [22] 80.0 80.3 86.2 86.3\\nFlorence [23] 80.2 80.4 - -\\nOFA Tiny 70.3 70.4 85.3 85.2\\nOFA Medium 75.4 75.5 86.6 87.0\\nOFA Base 78.0 78.1 89.3 89.2\\nOFA Large 80.3 80.5 90.3 90.2\\nOFA 82.0 82.0 91.0 91.2\\nTable 3: Experimental results on MSCOCO Image Captioning. We report the results on the Karpathy test split. Note', metadata={'source': 'OFA.pdf', 'page': 5}),\n",
       " Document(page_content='OFA Base 78.0 78.1 89.3 89.2\\nOFA Large 80.3 80.5 90.3 90.2\\nOFA 82.0 82.0 91.0 91.2\\nTable 3: Experimental results on MSCOCO Image Captioning. We report the results on the Karpathy test split. Note\\nthat SimVLM and LEMON are huge-size models.\\nModelCross-Entropy Optimization CIDEr Optimization\\nBLEU@4 METEOR CIDEr SPICE BLEU@4 METEOR CIDEr SPICE\\nVL-T5 [56] 34.5 28.7 116.5 21.9 - - - -\\nOSCAR [15] 37.4 30.7 127.8 23.5 41.7 30.6 140.0 24.5\\nUNICORN [57] 35.8 28.4 119.1 21.5 - - - -\\nVinVL [17] 38.5 30.4 130.8 23.4 41.0 31.1 140.9 25.2\\nUNIMO [46] 39.6 - 127.7 - - - - -\\nLEMON [71] 41.5 30.8 139.1 24.1 42.6 31.4 145.5 25.5\\nSimVLM [22] 40.6 33.7 143.3 25.4 - - - -\\nOFA Tiny 35.9 28.1 119.0 21.6 38.1 29.2 128.7 23.1\\nOFA Medium 39.1 30.0 130.4 23.2 41.4 30.8 140.7 24.8\\nOFA Base 41.0 30.9 138.2 24.2 42.8 31.7 146.7 25.8\\nOFA Large 42.4 31.5 142.2 24.5 43.6 32.2 150.7 26.2\\nOFA 43.9 31.8 145.3 24.8 44.9 32.5 154.9 26.6\\n4 Experiments', metadata={'source': 'OFA.pdf', 'page': 5}),\n",
       " Document(page_content='OFA Base 41.0 30.9 138.2 24.2 42.8 31.7 146.7 25.8\\nOFA Large 42.4 31.5 142.2 24.5 43.6 32.2 150.7 26.2\\nOFA 43.9 31.8 145.3 24.8 44.9 32.5 154.9 26.6\\n4 Experiments\\nThis section provides experimental details and analyses to demonstrate our model’s effectiveness. See Appendix A for\\nimplementation details.\\n4.1 Results on Cross-modal Tasks\\nWe evaluate our models on different cross-modal downstream tasks, covering cross-modal understanding and generation.\\nSpeciﬁcally, we implement experiments on multimodal understanding datasets including VQAv2 for visual question\\nanswering and SNLI-VE [ 73] for visual entailment, and multimodal generation including MSCOCO Image Caption [ 74]\\nfor image captioning, RefCOCO / RefCOCO+ / RefCOCOg [ 75,76] for referring expression comprehension as this\\n6', metadata={'source': 'OFA.pdf', 'page': 5}),\n",
       " Document(page_content='Table 4: Experimental results on the 3datasets of referring expression comprehension, namely RefCOCO, RefCOCO+,\\nand RefCOCOg. We report the Acc@0.5 on different test splits of the datasets.\\nModelRefCOCO RefCOCO+ RefCOCOg\\nval testA testB val testA testB val-u test-u\\nVL-T5 [56] - - - - - - - 71.3\\nUNITER [14] 81.41 87.04 74.17 75.90 81.45 66.70 74.86 75.77\\nVILLA [16] 82.39 87.48 74.84 76.17 81.54 66.84 76.18 76.71\\nMDETR [72] 86.75 89.58 81.41 79.52 84.09 70.62 81.64 80.89\\nUNICORN [57] 88.29 90.42 83.06 80.30 85.05 71.88 83.44 83.93\\nOFA Tiny 80.20 84.07 75.00 68.22 75.13 57.66 72.02 69.74\\nOFA Medium 85.34 87.68 77.92 76.09 83.04 66.25 78.76 78.58\\nOFA Base 88.48 90.67 83.30 81.39 87.15 74.29 82.29 82.31\\nOFA Large 90.05 92.93 85.26 85.80 89.87 79.22 85.89 86.55\\nOFA 92.04 94.03 88.44 87.86 91.70 80.71 88.07 88.78\\ntask can be viewed as bounding box generation, and MSCOCO Image Caption for text-to-image generation. More\\ndetails are provided in Appendix A.3.', metadata={'source': 'OFA.pdf', 'page': 6}),\n",
       " Document(page_content='OFA 92.04 94.03 88.44 87.86 91.70 80.71 88.07 88.78\\ntask can be viewed as bounding box generation, and MSCOCO Image Caption for text-to-image generation. More\\ndetails are provided in Appendix A.3.\\nTable 2 presents the performance of OFA and baseline models on VQA and SNLI-VE. In general, OFA achieves the\\nbest performance in both tasks with 82.0on the VQA test-std set and 91.2on the SNLI-VE test set. For smaller-size\\nmodels, OFA Large can outperform the recent SOTAs, e.g., VLMo and SimVLM, and OFA Base can beat the SOTAs\\nbefore the aforementioned two models in both tasks. This demonstrates that OFA can achieve superior performance on\\ncross-modal understanding tasks and scaling up OFA can bring signiﬁcant improvements, reﬂecting the strong potential\\nof large-scale pretrained models.\\nTable 3 presents the performance of OFA and baseline models on the MSCOCO image captioning dataset. We report', metadata={'source': 'OFA.pdf', 'page': 6}),\n",
       " Document(page_content='of large-scale pretrained models.\\nTable 3 presents the performance of OFA and baseline models on the MSCOCO image captioning dataset. We report\\nthe results on the Karpathy test split, and we demonstrate the performance of models trained with Cross-Entropy\\noptimization and additionally with CIDEr optimization based on reinforcement learning. In comparison with the\\nprevious SOTA SimVLM Huge for Cross-Entropy optimization, OFA outperforms it by around 2points in CIDEr\\nevaluation. For CIDEr optimization, OFA of the 3sizes all outperform the huge-size LEMON, and OFA demonstrates a\\nnew SOTA of 154.9CIDEr score. By May 31 2022, the single-model OFA had topped the MSCOCO Image Caption\\nLeaderboard.3\\nTo evaluate the capability of visual grounding, we conduct experiments on RefCOCO, RefCOCO+, and RefCOCOg.\\nWhile we unify locations to the vocabulary, visual grounding can be viewed as a sequence generation task. As', metadata={'source': 'OFA.pdf', 'page': 6}),\n",
       " Document(page_content='While we unify locations to the vocabulary, visual grounding can be viewed as a sequence generation task. As\\nthere is only one target for each query, we limit the generation length to 4in order to generate a bounding box by\\n<x1,y1,x2,y2>. Experimental results in Table 4 show that OFA reaches the SOTA performance on the 3datasets.\\nCompared with the previous SOTA UNICORN [ 57], OFA achieves signiﬁcant improvement with a gain of 3.61,6.65\\nand4.85points on the testA sets of RefCOCO and RefCOCO+ as well as the test-u set of RefCOCOg.\\nText-to-image generation is a challenging task even for pretrained model. As we pretrain OFA with the task “image-\\ninﬁlling”, i.e., recovering masked patches by generating the corresponding codes [ 36], and thus OFA is able to generate\\ncode. We thus directly ﬁnetune OFA on the MSCOCO Image Caption dataset for text-to-code generation. At the\\ninference stage, we additionally transform the generated codes to an image with the code decoder. Speciﬁcally, we use', metadata={'source': 'OFA.pdf', 'page': 6}),\n",
       " Document(page_content='inference stage, we additionally transform the generated codes to an image with the code decoder. Speciﬁcally, we use\\nthe codes from VQGAN [ 54] following [ 52]. Experimental results show that OFA outperforms the baselines in all the\\nmetrics. Note that increasing the sampling size during inference is expected to bring clear improvements on FID and IS.\\nCompared with DALLE [ 50], CogView [ 51] and NÜWA [ 52], whose sampling sizes are 512,60and60, respectively,\\nOFA outperforms these SOTA methods on FID and IS with a much smaller sampling size 24. This illustrates that\\nOFA has learned better correspondence among the query text, the image and the image codes.\\nWe compare OFA with CogView and GLIDE on generation quality with normal and counterfactual queries.4Normal\\nqueries describe existing things in the real world, while counterfactual queries refer to those describing things that could', metadata={'source': 'OFA.pdf', 'page': 6}),\n",
       " Document(page_content='queries describe existing things in the real world, while counterfactual queries refer to those describing things that could\\nonly exist in our imagination. For normal queries, both CogView and OFA generate images semantically consistent with\\nthe given texts, in comparison with GLIDE. The generated examples from our model can provide more sophisticated\\n3https://competitions.codalab.org/competitions/3221#results\\n4For more implementation details, please refer to Appendix A.3\\n7', metadata={'source': 'OFA.pdf', 'page': 6}),\n",
       " Document(page_content='GLIDEOFACogViewNormal QueryCounterfactual Query\\nA brown horse in the street.A banana in the shape of the bird.Cattle grazing on grass near a lake surrounded by mountain.A street scene with a double-decker bus on the road.An orange clock in the water.A white computer in the sky.\\nFigure 3: Qualitative comparison with state-of-the-art models for text-to-image generation task. We present more\\nqualitative examples of text-to-image generation for better demonstration in Appendix C.\\nTable 5: Experimental results on text-to-image generation. Models are evaluated on FID, CLIPSIM, and IS scores.\\nOFA outperforms the baselines, including the concurrent SOTA NÜWA. We report the results of OFA Large . Note that\\nGLIDE additionally has 1.5Bparameters for upsampling except for the 3.5Bparameters.\\nModel FID ↓CLIPSIM↑ IS↑\\nDALLE [50] 27.5 - 17.9\\nCogView [51] 27.1 33.3 18.2\\nGLIDE [77] 12.2 - -\\nUnifying [78] 29.9 30.9 -\\nNÜWA [52] 12.9 34.3 27.2\\nOFA 10.5 34.4 31.1', metadata={'source': 'OFA.pdf', 'page': 7}),\n",
       " Document(page_content='Model FID ↓CLIPSIM↑ IS↑\\nDALLE [50] 27.5 - 17.9\\nCogView [51] 27.1 33.3 18.2\\nGLIDE [77] 12.2 - -\\nUnifying [78] 29.9 30.9 -\\nNÜWA [52] 12.9 34.3 27.2\\nOFA 10.5 34.4 31.1\\ndetails of objects, say the horse and the double-decker bus. For counterfactual queries, we ﬁnd that OFA is the only one\\nthat can generate the three imaginary scenes, which indicates its imaginative power based on its strong capability to\\nalign text to the image. See Appendix C for more qualitative examples.\\n4.2 Results on Uni-modal Tasks\\nAs the design of OFA uniﬁes different modalities, we evaluate its performance on unimodal tasks, namely tasks\\nof natural language and computer vision. For natural language tasks, we evaluate OFA on 6tasks of the GLUE\\nbenchmark [ 79] for natural language understanding and Gigaword abstractive summarization [ 80] for natural language\\ngeneration. For computer vision, we evaluate OFA on the classic ImageNet-1K [ 81] dataset for image classiﬁcation.\\nMore details are provided in Appendix A.3.', metadata={'source': 'OFA.pdf', 'page': 7}),\n",
       " Document(page_content='generation. For computer vision, we evaluate OFA on the classic ImageNet-1K [ 81] dataset for image classiﬁcation.\\nMore details are provided in Appendix A.3.\\nAs OFA has been pretrained on plain text data, it can be directly transferred to natural language downstream tasks.\\nFor natural language generation, it is essentially a sequence-to-sequence generation task, and for natural language\\n8', metadata={'source': 'OFA.pdf', 'page': 7}),\n",
       " Document(page_content='Table 6: Experimental results on the GLUE benchmark datasets [ 79]. For comparison, we list the performance of\\nmultimodal pretrained models as well the recent SOTA models that were pretrained on natural language data only.\\nFollowing [28], we ﬁnetune RTE and MRPC starting from the checkpoint ﬁnetuned on MNLI.\\nModel SST-2 RTE MRPC QQP MNLI QNLI\\nMultimodal Pretrained Baseline Models\\nVisualBERT [38] 89.4 56.6 71.9 89.4 81.6 87.0\\nUNITER [14] 89.7 55.6 69.3 89.2 80.9 86.0\\nVL-BERT [8] 89.8 55.7 70.6 89.0 81.2 86.3\\nVilBERT [13] 90.4 53.7 69.0 88.6 79.9 83.8\\nLXMERT [40] 90.2 57.2 69.8 75.3 80.4 84.2\\nUni-Perceiver [61] 90.2 64.3 86.6 87.1 81.7 89.9\\nSimVLM [22] 90.9 63.9 75.2 90.4 83.4 88.6\\nFLA V A [60] 90.9 57.8 81.4 90.4 80.3 87.3\\nUNIMO [46] 96.8 - - - 89.8 -\\nNatural-Language-Pretrained SOTA Models\\nBERT [2] 93.2 70.4 88.0 91.3 86.6 92.3\\nRoBERTa [28] 96.4 86.6 90.9 92.2 90.2 93.9\\nXLNet [25] 97.0 85.9 90.8 92.3 90.8 94.9\\nELECTRA [82] 96.9 88.0 90.8 92.4 90.9 95.0', metadata={'source': 'OFA.pdf', 'page': 8}),\n",
       " Document(page_content='BERT [2] 93.2 70.4 88.0 91.3 86.6 92.3\\nRoBERTa [28] 96.4 86.6 90.9 92.2 90.2 93.9\\nXLNet [25] 97.0 85.9 90.8 92.3 90.8 94.9\\nELECTRA [82] 96.9 88.0 90.8 92.4 90.9 95.0\\nDeBERTa [83] 96.8 88.3 91.9 92.3 91.1 95.3\\nOurs\\nOFA 96.6 91.0 91.7 92.5 90.2 94.8\\nTable 7: Experimental results on Gigaword abstractive summarization. We report performance on the ROUGE\\nevaluation [84].\\nModelGigaword\\nROUGE-1 ROUGE-2 ROUGE-L\\nBERTSHARE [85] 38.13 19.81 35.62\\nMASS [86] 38.73 19.71 35.96\\nUniLM [29] 38.45 19.45 35.75\\nPEGASUS [87] 39.12 19.86 36.24\\nProphetNet [88] 39.55 20.27 36.57\\nUNIMO [46] 39.71 20.37 36.88\\nOFA 39.81 20.66 37.11\\nunderstanding, typically text classiﬁcation, we regard them as generation tasks where labels are essentially word\\nsequences. Additionally, for each task, we design a manual instruction to indicate the model what types of questions it\\nshould answer. We list our instruction design in Appendix A.3.', metadata={'source': 'OFA.pdf', 'page': 8}),\n",
       " Document(page_content='sequences. Additionally, for each task, we design a manual instruction to indicate the model what types of questions it\\nshould answer. We list our instruction design in Appendix A.3.\\nWe demonstrate that even a uniﬁed multimodal pretrained model can achieve highly competitive performance in natural\\nlanguage tasks. Speciﬁcally, in the evaluation of natural language understanding, OFA surpasses multimodal pretrained\\nmodels by large margins in all tasks. In comparison with the state-of-the-art natural language pretrained models,\\nincluding RoBERTa [ 28], XLNET [ 25], ELECTRA [ 82], and DeBERTa [ 83], OFA reaches a comparable performance.\\nIn the evaluation of natural language generation, OFA even reaches a new state-of-the-art performance on the Gigaword\\ndataset.\\nAlso, OFA can reach a competitive performance in image classiﬁcation. Table 8 shows the performance of OFA on\\nimage classiﬁcation. OFA Large achieves higher accuracy than previous backbone models such as EfﬁcientNet-B7 [ 89]', metadata={'source': 'OFA.pdf', 'page': 8}),\n",
       " Document(page_content='image classiﬁcation. OFA Large achieves higher accuracy than previous backbone models such as EfﬁcientNet-B7 [ 89]\\nand ViT-L [ 6]. We also compare OFA with self-supervised pretraining models based on contrastive learning and masked\\nimage modeling. OFA outperforms contrastive-based models such as SimCLR [ 32] and MoCo-v3 [ 33,35] with similar\\nparameters. Compared with pretrained models based on masked image modeling, e.g., BEiT-L [ 36] and MAE-L [ 37],\\nOFA can achieve similar performance.\\n9', metadata={'source': 'OFA.pdf', 'page': 8}),\n",
       " Document(page_content='Table 8: ImageNet-1K ﬁnetuning results. All the listed models do not use extra labeled image classiﬁcation samples\\nduring training for fair comparison. We report the results of OFA Large .\\nModel Top-1 Acc.\\nEfﬁcientNet-B7 [89] 84.3\\nViT-L/16 [6] 82.5\\nDINO [90] 82.8\\nSimCLR v2 [32] 82.9\\nMoCo v3 [35] 84.1\\nBEiT 384-L/16 [36] 86.3\\nMAE-L/16 [37] 85.9\\nOFA 85.6\\nTable 9: Zero-shot performance on 6GLUE tasks and SNLI-VE.\\nModelSST-2 RTE MRPC QQP QNLI MNLI SNLI-VE\\nAcc. Acc. F1 F1 Acc. Acc. Acc. (dev/test)\\nUni-Perceiver 70.6 55.6 76.1 53.6 51.0 49.6 -\\nOFA Base 71.6 56.7 79.5 54.0 51.4 37.3 49.71 / 49.18\\nThese aforementioned results in both natural language and vision tasks indicate that a uniﬁed multimodal pretrained\\nmodel is not only effective in multimodal tasks but also capable of tackling unimodal tasks, and in the future, it might\\nbe sufﬁcient for such a model to solve complex tasks concerning different modality combinations.\\n4.3 Zero-shot Learning & Task Transfer', metadata={'source': 'OFA.pdf', 'page': 9}),\n",
       " Document(page_content='be sufﬁcient for such a model to solve complex tasks concerning different modality combinations.\\n4.3 Zero-shot Learning & Task Transfer\\nThe instruction-guided pretraining enables OFA to perform zero-shot inference. Following Uni-Perceiver [ 61], we\\nevaluate our model on the 6tasks of the GLUE benchmark, including single-sentence classiﬁcation and sentence pair\\nclassiﬁcation. Table 9 demonstrates that OFA generally outperforms Uni-Perceiver. However, both models do not\\nachieve satisfactory performance in sentence-pair classiﬁcation (with Acc.<60%). We hypothesize that the missing\\nsentence-pair data in the pretraining dataset attributes to the performance.\\nAlso, we ﬁnd that the model performance is highly sensitive to the design of instructions. To obtain the best result,\\none should search a proper instruction template possibly from a large pool of candidates. A slight change to manual', metadata={'source': 'OFA.pdf', 'page': 9}),\n",
       " Document(page_content='one should search a proper instruction template possibly from a large pool of candidates. A slight change to manual\\nprompts or model parameters may drastically inﬂuence the model performance, which is not robust. We leave this issue\\nto the future work.\\nWe observe that the model can transfer to unseen tasks well with new task instructions. We design a new task called\\ngrounded question answering and present examples in Figure 4. In this scenario, given a question about a certain region\\non the image, the model should provide a correct answer. We ﬁnd that the model can achieve a satisfactory performance\\nin this new task, which reﬂects its strong transferability. Besides, OFA can solve tasks with the out-of-domain input\\ndata. For example, OFA without ﬁnetuning achieves satisfactory performance in VQA for the out-of-domain images.\\nExamples are demonstrated in Figure 5. OFA can also perform accurate visual grounding on the out-of-domain images,', metadata={'source': 'OFA.pdf', 'page': 9}),\n",
       " Document(page_content='Examples are demonstrated in Figure 5. OFA can also perform accurate visual grounding on the out-of-domain images,\\ne.g., anime pictures, synthetic images, etc., and we demonstrate more examples on Figure 11 in Appendix C.\\n4.4 Ablation on Multitask Pretraining\\nThanks to the uniﬁed framework, OFA has been pretrained on multiple tasks and thus endowed with comprehensive\\ncapabilities. However, the effects of each task are still undiscovered. We verify their effects on multiple downstream\\ntasks, including image captioning, VQA, image classiﬁcation, and text-to-image generation.\\nWe ﬁrst evaluate how uni-modal pretraining tasks inﬂuence the performance in both cross-modal and uni-modal\\ntasks. Table 10 demonstrates our experimental results. We observe some interesting phenomena about the effects of\\nuni-modal pretraining tasks. Text inﬁlling brings improvement on image caption ( +0.8CIDEr) and VQA ( +0.46Acc.).', metadata={'source': 'OFA.pdf', 'page': 9}),\n",
       " Document(page_content='uni-modal pretraining tasks. Text inﬁlling brings improvement on image caption ( +0.8CIDEr) and VQA ( +0.46Acc.).\\nNatural language pretraining betters the contextualized representation of language and thus enhances performance in\\ncross-modal tasks. However, it is noticed that the language pretraining task may degrade the performance in image\\n10', metadata={'source': 'OFA.pdf', 'page': 9}),\n",
       " Document(page_content='what color is the car in the region? region: <loc301> <loc495> <loc501> <loc596>what color is the car in the region? region: <loc512> <loc483> <loc675> <loc576>tangrayQ:A:A:Q:Figure 4: Qualitative results on an unseen task grounded QA. We design a new task called grounded question answering,\\nwhere the model should answer a question about a certain region in the image. More samples are provided in Figure 10\\nin Appendix C.\\nwhat is grown on the plant?moneyQ:A:what does the red-roofed building right to the big airship look like?D\\x03PXVKURRPA:Q:\\nFigure 5: Qualitative results on unseen domain VQA. During pretraining, only real-world photographs are used for\\nVQA. We present cases of VQA on out-of-domain images, i.e., the iconic and sci-ﬁ images, and demonstrate their\\ncapability of transferring to unseen domains. More samples are provided in Figure 9 in Appendix C.\\nclassiﬁcation, leading to the decrease in ImageNet-1K ( −1.0Acc.). Also, it is interesting to ﬁnd that it does not', metadata={'source': 'OFA.pdf', 'page': 10}),\n",
       " Document(page_content='classiﬁcation, leading to the decrease in ImageNet-1K ( −1.0Acc.). Also, it is interesting to ﬁnd that it does not\\nencourage improvement in text-to-image generation ( −0.1CLIPSIM). It may attribute to the simplicity of text in\\nthis task, which indicates that improved representation of language does not affect the performance. As to image\\ninﬁlling, it signiﬁcantly improves the performance in image classiﬁcation ( +1.0Acc.) and text-to-image generation\\n(+0.6CLIPSIM). Learning to recover images is an effective self-supervised task for image representation, and it also\\nencourages the decoder’s ability to generate image codes. However, it hurts the performance in image captioning and\\nVQA. Both tasks require a strong capability in generating texts, and the decoder’s learning of image generation naturally\\nbrings performance degradation in captioning ( −0.7CIDEr) and VQA ( −0.3Acc.).\\nFurthermore, we evaluate how multimodal tasks impact the performance. Previous studies have provided evidence', metadata={'source': 'OFA.pdf', 'page': 10}),\n",
       " Document(page_content='brings performance degradation in captioning ( −0.7CIDEr) and VQA ( −0.3Acc.).\\nFurthermore, we evaluate how multimodal tasks impact the performance. Previous studies have provided evidence\\nof the contribution of conventional pretraining tasks, e.g., MLM, MOC, ITM, VQA, image captioning, etc. [ 14,17].\\nHowever, they miss other tasks, e.g., detection and visual grounding & grounded captioning. We conduct experiments\\non these tasks and ﬁnd that tasks predicting regions are crucial to multimodal tasks, with a performance increase in\\nimage captioning ( +2.3CIDEr & +1.4CIDEr) and VQA ( +0.6Acc. & +0.5Acc.). It suggests that detection and\\nvisual grounding & grounded captioning help the model grasp ﬁned-grained alignments between vision and language.\\n11', metadata={'source': 'OFA.pdf', 'page': 10}),\n",
       " Document(page_content='Table 10: Ablation results of OFA. All models are pretrained for 250ksteps. w/o ground. represents the removal of\\nboth visual grounding and grounded captioning tasks. Note that all models are only ﬁnetuned with the cross-entropy\\nloss in image captioning.\\nModelCaption VQA ImageNet Image Generation\\nCIDEr Test-dev Top-1 Acc. FID / CLIPSIM / IS\\nOFA Base 135.6 76.0 82.2 20.8 / 31.6 / 21.5\\nw/o text inﬁll. 134.8 75.6 83.2 20.3 / 31.7 / 21.8\\nw/o image inﬁll. 136.3 76.3 81.8 23.2 / 31.0 / 20.0\\nw/o det. 133.3 75.4 81.4 20.9 / 31.5 / 21.6\\nw/o ground. 134.2 75.5 82.0 21.2 / 31.5 / 21.5\\nRegion information contributes little to text-to-image generation ( +0.1CLIPSIM & +0.1CLIPSIM), as this task\\nrequires far less text-region alignment information. We surprisingly ﬁnd that detection can encourage the performance\\nin visual understanding ( +0.8Acc.). It indicates that incorporating region information might be essential to visual\\nunderstanding, especially on images with complex objects.\\n5 Conclusion', metadata={'source': 'OFA.pdf', 'page': 11}),\n",
       " Document(page_content='in visual understanding ( +0.8Acc.). It indicates that incorporating region information might be essential to visual\\nunderstanding, especially on images with complex objects.\\n5 Conclusion\\nIn this work, we propose OFA , a Task-Agnostic and Modality-Agnostic framework supporting Task Comprehensiveness.\\nOFA achieves the uniﬁcation in architecture, tasks and modalities, and thus is capable of multimodal & uni-modal\\nunderstanding and generation, without speciﬁcation in additional layers or tasks. Our experiments show that OFA creates\\nnew SOTAs in a series of tasks, including image captioning, VQA, visual entailment, and referring expression\\ncomprehension. OFA also demonstrates a comparable performance with language / vision pretrained SOTA models in\\nuni-modal understanding and generation tasks, e.g., GLUE, abstractive summarization, and image classiﬁcation. We\\nprovide a further analysis to demonstrate its capability in zero-shot learning and domain & task transfer, and we also', metadata={'source': 'OFA.pdf', 'page': 11}),\n",
       " Document(page_content='provide a further analysis to demonstrate its capability in zero-shot learning and domain & task transfer, and we also\\nverify the effectiveness of pretraining tasks.\\nIn the future, we will continue exploring the issues discovered in this work. Also, we endeavor to ﬁgure out a reasonable\\nsolution to building an omni-model essentially generalizable to the complex real world.\\nAcknowledgments\\nWe would like to thank Jie Zhang, Yong Li, Jiamang Wang, Shao Yuan, and Zheng Cao for their support to this project,\\nand we would like to thank Guangxiang Zhao and Fei Sun for their insightful comments to our paper.\\nReferences\\n[1]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need. In NeurIPS 2017, pages 5998–6008, 2017.\\n[2]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirec-', metadata={'source': 'OFA.pdf', 'page': 11}),\n",
       " Document(page_content='and Illia Polosukhin. Attention is all you need. In NeurIPS 2017, pages 5998–6008, 2017.\\n[2]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirec-\\ntional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors,\\nNAACL-HLT 2019, pages 4171–4186. Association for Computational Linguistics, 2019.\\n[3]Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv\\npreprint arXiv:2005.14165, 2020.\\n[4]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and\\nJohn Schulman. Training veriﬁers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n[5]Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised pre-training for', metadata={'source': 'OFA.pdf', 'page': 11}),\n",
       " Document(page_content='[5]Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised pre-training for\\nspeech recognition. arXiv preprint arXiv:1904.05862, 2019.\\n[6]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:\\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n12', metadata={'source': 'OFA.pdf', 'page': 11}),\n",
       " Document(page_content='[7]Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver:\\nGeneral perception with iterative attention. arXiv preprint arXiv:2103.03206, 2021.\\n[8]Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic\\nvisual-linguistic representations. In International Conference onLearning Representations, 2019.\\n[9]Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai,\\nand Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\\n[10] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaf-\\nﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task\\ngeneralization. arXiv preprint arXiv:2110.08207, 2021.\\n[11] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo,', metadata={'source': 'OFA.pdf', 'page': 12}),\n",
       " Document(page_content='generalization. arXiv preprint arXiv:2110.08207, 2021.\\n[11] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo,\\nMona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp. In International Conference on\\nMachine Learning, pages 2790–2799. PMLR, 2019.\\n[12] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning.\\narXiv preprint arXiv:2104.08691, 2021.\\n[13] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic represen-\\ntations for vision-and-language tasks. In NeurIPS, 2019.\\n[14] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\\nUniter: Universal image-text representation learning. In ECCV, 2020.\\n[15] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong,', metadata={'source': 'OFA.pdf', 'page': 12}),\n",
       " Document(page_content='Uniter: Universal image-text representation learning. In ECCV, 2020.\\n[15] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong,\\nFuru Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks.\\nInECCV, 2020.\\n[16] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for\\nvision-and-language representation learning. ArXiv, abs/2006.06195, 2020.\\n[17] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.\\nVinvl: Revisiting visual representations in vision-language models. 2021 IEEE/CVF Conference onComputer\\nVision andPattern Recognition (CVPR), pages 5575–5584, 2021.\\n[18] Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang,\\nXianyan Jia, et al. M6: A chinese multimodal pretrainer. arXiv preprint arXiv:2103.00823, 2021.', metadata={'source': 'OFA.pdf', 'page': 12}),\n",
       " Document(page_content='Xianyan Jia, et al. M6: A chinese multimodal pretrainer. arXiv preprint arXiv:2103.00823, 2021.\\n[19] Zhu Zhang, Jianxin Ma, Chang Zhou, Rui Men, Zhikang Li, Ming Ding, Jie Tang, Jingren Zhou, and Hongxia\\nYang. M6-ufc: Unifying multi-modal controls for conditional image synthesis. arXiv preprint arXiv:2105.14211 ,\\n2021.\\n[20] An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang,\\nYong Li, et al. Exploring sparse expert models and beyond. arXiv preprint arXiv:2105.15082, 2021.\\n[21] Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin,\\net al. M6-10t: A sharing-delinking paradigm for efﬁcient multi-trillion parameter pretraining. arXiv preprint\\narXiv:2110.03888, 2021.\\n[22] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual\\nlanguage model pretraining with weak supervision. ArXiv, abs/2108.10904, 2021.', metadata={'source': 'OFA.pdf', 'page': 12}),\n",
       " Document(page_content='[22] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual\\nlanguage model pretraining with weak supervision. ArXiv, abs/2108.10904, 2021.\\n[23] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel C. F. Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong\\nHuang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng\\nWang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and Pengchuan Zhang. Florence: A\\nnew foundation model for computer vision. ArXiv, abs/2111.11432, 2021.\\n[24] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\\nstanding by generative pre-training. URL https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/\\nlanguageunsupervised/language understanding paper. pdf, 2018.\\n[25] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. Xlnet:', metadata={'source': 'OFA.pdf', 'page': 12}),\n",
       " Document(page_content='languageunsupervised/language understanding paper. pdf, 2018.\\n[25] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. Xlnet:\\nGeneralized autoregressive pretraining for language understanding. In NeurIPS 2019, pages 5754–5764, 2019.\\n[26] Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian,\\nand Hua Wu. ERNIE: enhanced representation through knowledge integration. CoRR, abs/1904.09223, 2019.\\n[27] Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE 2.0: A\\ncontinual pre-training framework for language understanding. CoRR, abs/1907.12412, 2019.\\n13', metadata={'source': 'OFA.pdf', 'page': 12}),\n",
       " Document(page_content='[28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR ,\\nabs/1907.11692, 2019.\\n[29] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-\\nWuen Hon. Uniﬁed language model pre-training for natural language understanding and generation. In NeurIPS\\n2019, pages 13042–13054, 2019.\\n[30] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\\nLi, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of\\nMachine Learning Research, 21(140):1–67, 2020.\\n[31] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin\\nStoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language', metadata={'source': 'OFA.pdf', 'page': 13}),\n",
       " Document(page_content='Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language\\ngeneration, translation, and comprehension. In ACL 2020, July 2020.\\n[32] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive\\nlearning of visual representations. In International conference onmachine learning , pages 1597–1607. PMLR,\\n2020.\\n[33] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive\\nlearning. arXiv preprint arXiv:2003.04297, 2020.\\n[34] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl\\nDoersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own\\nlatent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.\\n[35] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings ofthe', metadata={'source': 'OFA.pdf', 'page': 13}),\n",
       " Document(page_content='latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.\\n[35] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings ofthe\\nIEEE/CVF Conference onComputer Vision andPattern Recognition, pages 15750–15758, 2021.\\n[36] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\\narXiv:2106.08254, 2021.\\n[37] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are\\nscalable vision learners. arXiv preprint arXiv:2111.06377, 2021.\\n[38] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and\\nperformant baseline for vision and language. ArXiv, abs/1908.03557, 2019.\\n[39] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao. Uniﬁed vision-\\nlanguage pre-training for image captioning and VQA. In AAAI 2020, pages 13041–13049, 2020.', metadata={'source': 'OFA.pdf', 'page': 13}),\n",
       " Document(page_content='[39] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao. Uniﬁed vision-\\nlanguage pre-training for image captioning and VQA. In AAAI 2020, pages 13041–13049, 2020.\\n[40] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers.\\nInProceedings ofthe2019 Conference onEmpirical Methods inNatural Language Processing andthe9th\\nInternational Joint Conference onNatural Language Processing (EMNLP-IJCNLP), pages 5100–5111, 2019.\\n[41] Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou. Unicoder-vl: A universal encoder for vision and\\nlanguage by cross-modal pre-training. CoRR, abs/1908.06066, 2019.\\n[42] Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, and Hongxia Yang. Interbert: Vision-and-language\\ninteraction for multi-modal pretraining. arXiv preprint arXiv:2003.13198, 2020.\\n[43] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-task vision and', metadata={'source': 'OFA.pdf', 'page': 13}),\n",
       " Document(page_content='interaction for multi-modal pretraining. arXiv preprint arXiv:2003.13198, 2020.\\n[43] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-task vision and\\nlanguage representation learning. In Proceedings oftheIEEE/CVF Conference onComputer Vision andPattern\\nRecognition, pages 10437–10446, 2020.\\n[44] Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, and Fei Huang. E2e-vlp:\\nEnd-to-end vision-language pre-training enhanced by visual learning. arXiv preprint arXiv:2106.01804, 2021.\\n[45] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil: Knowledge enhanced\\nvision-language representations through scene graphs. In Proceedings oftheAAAI Conference onArtiﬁcial\\nIntelligence, volume 35, pages 3208–3216, 2021.\\n[46] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. UNIMO:', metadata={'source': 'OFA.pdf', 'page': 13}),\n",
       " Document(page_content='Intelligence, volume 35, pages 3208–3216, 2021.\\n[46] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. UNIMO:\\ntowards uniﬁed-modal understanding and generation via cross-modal contrastive learning. In Chengqing\\nZong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, ACL/IJCNLP 2021, pages 2592–2607. Association for\\nComputational Linguistics, 2021.\\n[47] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels\\nwith text by deep multi-modal transformers. ArXiv, abs/2004.00849, 2020.\\n[48] Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. Vlmo: Uniﬁed vision-language pre-training with mixture-\\nof-modality-experts. ArXiv, abs/2111.02358, 2021.\\n14', metadata={'source': 'OFA.pdf', 'page': 13}),\n",
       " Document(page_content='[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual\\nmodels from natural language supervision. In Marina Meila and Tong Zhang, editors, ICML 2021 , volume 139\\nofProceedings ofMachine Learning Research, pages 8748–8763. PMLR, 2021.\\n[50] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya\\nSutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\\n[51] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\\nShao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. arXiv preprint\\narXiv:2105.13290, 2021.\\n[52] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N \\\\\" uwa: Visual synthesis', metadata={'source': 'OFA.pdf', 'page': 14}),\n",
       " Document(page_content='arXiv:2105.13290, 2021.\\n[52] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N \\\\\" uwa: Visual synthesis\\npre-training for neural visual world creation. arXiv preprint arXiv:2111.12417, 2021.\\n[53] Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NIPS ,\\n2017.\\n[54] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In\\nProceedings oftheIEEE/CVF Conference onComputer Vision andPattern Recognition , pages 12873–12883,\\n2021.\\n[55] Lukasz Kaiser, Aidan N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, and Jakob Uszkoreit.\\nOne model to learn them all. arXiv preprint arXiv:1706.05137, 2017.\\n[56] Jaemin Cho, Jie Lei, Haochen Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation.\\nInICML, 2021.\\n[57] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan', metadata={'source': 'OFA.pdf', 'page': 14}),\n",
       " Document(page_content='InICML, 2021.\\n[57] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan\\nWang. Crossing the format boundary of text and boxes: Towards uniﬁed vision-language modeling. ArXiv ,\\nabs/2111.12085, 2021.\\n[58] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda\\nKoppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture for structured\\ninputs & outputs. arXiv preprint arXiv:2107.14795, 2021.\\n[59] Ronghang Hu and Amanpreet Singh. Unit: Multimodal multitask learning with a uniﬁed transformer. arXiv\\npreprint arXiv:2102.10772, 2021.\\n[60] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach,\\nand Douwe Kiela. Flava: A foundational language and vision alignment model. arXiv preprint arXiv:2112.04482 ,\\n2021.\\n[61] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Xiaogang Wang, Hongsheng Li, Xiaohua Wang, and Jifeng Dai.', metadata={'source': 'OFA.pdf', 'page': 14}),\n",
       " Document(page_content='2021.\\n[61] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Xiaogang Wang, Hongsheng Li, Xiaohua Wang, and Jifeng Dai.\\nUni-perceiver: Pre-training uniﬁed architecture for generic perception for zero-shot and few-shot tasks. arXiv\\npreprint arXiv:2112.01522, 2021.\\n[62] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all\\ndata sizes. arXiv preprint arXiv:2106.04803, 2021.\\n[63] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword\\nunits. In Proceedings ofthe54th Annual Meeting oftheAssociation forComputational Linguistics (V olume 1:\\nLong Papers), pages 1715–1725, 2016.\\n[64] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language modeling\\nframework for object detection. arXiv preprint arXiv:2109.10852, 2021.\\n[65] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR , abs/1607.06450, 2016.', metadata={'source': 'OFA.pdf', 'page': 14}),\n",
       " Document(page_content='framework for object detection. arXiv preprint arXiv:2109.10852, 2021.\\n[65] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR , abs/1607.06450, 2016.\\n[66] Sam Shleifer, Jason Weston, and Myle Ott. Normformer: Improved transformer pretraining with extra normaliza-\\ntion. arXiv preprint arXiv:2110.09456, 2021.\\n[67] Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In International\\nConference onLearning Representations, 2020.\\n[68] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. Introduction toalgorithms . MIT\\npress, 2009.\\n[69] Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shaﬁq Joty, Caiming Xiong, and Steven Hoi.\\nAlign before fuse: Vision and language representation learning with momentum distillation. In Thirty-Fifth\\nConference onNeural Information Processing Systems, 2021.\\n15', metadata={'source': 'OFA.pdf', 'page': 14}),\n",
       " Document(page_content='[70] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Nanyun Peng,\\nZicheng Liu, and Michael Zeng. An empirical study of training end-to-end vision-and-language transformers.\\nArXiv, abs/2111.02387, 2021.\\n[71] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up\\nvision-language pre-training for image captioning. CoRR, abs/2111.12233, 2021.\\n[72] Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan Misra, Gabriel Synnaeve, and Nicolas Carion. Mdetr -\\nmodulated detection for end-to-end multi-modal understanding. ArXiv, abs/2104.12763, 2021.\\n[73] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: A novel task for ﬁne-grained image\\nunderstanding. arXiv preprint arXiv:1901.06706, 2019.\\n[74] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence', metadata={'source': 'OFA.pdf', 'page': 15}),\n",
       " Document(page_content='understanding. arXiv preprint arXiv:1901.06706, 2019.\\n[74] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence\\nZitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 , 2015.\\n[75] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring\\nexpressions. In European Conference onComputer Vision, pages 69–85. Springer, 2016.\\n[76] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation\\nand comprehension of unambiguous object descriptions. In Proceedings oftheIEEE conference oncomputer\\nvision andpattern recognition, pages 11–20, 2016.\\n[77] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever,\\nand Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models.\\narXiv preprint arXiv:2112.10741, 2021.', metadata={'source': 'OFA.pdf', 'page': 15}),\n",
       " Document(page_content='and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models.\\narXiv preprint arXiv:2112.10741, 2021.\\n[78] Yupan Huang, Hongwei Xue, Bei Liu, and Yutong Lu. Unifying multimodal transformer for bi-directional\\nimage and text generation. In Proceedings ofthe29th ACM International Conference onMultimedia , pages\\n1138–1147, 2021.\\n[79] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-\\ntask benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461 ,\\n2018.\\n[80] Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence\\nsummarization. In Proceedings ofthe2015 Conference onEmpirical Methods inNatural Language Processing ,\\npages 379–389, 2015.\\n[81] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical', metadata={'source': 'OFA.pdf', 'page': 15}),\n",
       " Document(page_content='pages 379–389, 2015.\\n[81] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\\nimage database. In 2009 IEEE conference oncomputer vision andpattern recognition , pages 248–255. Ieee,\\n2009.\\n[82] Kevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. ELECTRA: pre-training text\\nencoders as discriminators rather than generators. In 8thInternational Conference onLearning Representations,\\nICLR 2020. OpenReview.net, 2020.\\n[83] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: decoding-enhanced bert with disen-\\ntangled attention. In 9thInternational Conference onLearning Representations, ICLR 2021 . OpenReview.net,\\n2021.\\n[84] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches\\nOut, Barcelona, Spain, July 2004. Association for Computational Linguistics.', metadata={'source': 'OFA.pdf', 'page': 15}),\n",
       " Document(page_content='2021.\\n[84] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches\\nOut, Barcelona, Spain, July 2004. Association for Computational Linguistics.\\n[85] Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. Leveraging pre-trained checkpoints for sequence generation\\ntasks. Transactions oftheAssociation forComputational Linguistics, 8:264–280, 2020.\\n[86] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: masked sequence to sequence pre-training\\nfor language generation. In ICML 2019, pages 5926–5936, 2019.\\n[87] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. Pegasus: Pre-training with extracted gap-sentences\\nfor abstractive summarization. In International Conference onMachine Learning , pages 11328–11339. PMLR,\\n2020.\\n[88] Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou.\\nProphetnet: Predicting future n-gram for sequence-to-sequence pre-training. In Proceedings ofthe2020', metadata={'source': 'OFA.pdf', 'page': 15}),\n",
       " Document(page_content='Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training. In Proceedings ofthe2020\\nConference onEmpirical Methods inNatural Language Processing: Findings, pages 2401–2410, 2020.\\n[89] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. In\\nInternational Conference onMachine Learning, pages 6105–6114. PMLR, 2019.\\n[90] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.\\nEmerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294, 2021.\\n16', metadata={'source': 'OFA.pdf', 'page': 15}),\n",
       " Document(page_content='[91] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale\\nimage-text pre-training to recognize long-tail visual concepts. In Proceedings oftheIEEE/CVF Conference on\\nComputer Vision andPattern Recognition, pages 3558–3568, 2021.\\n[92] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed,\\nimage alt-text dataset for automatic image captioning. In ACL 2018, pages 2556–2565, 2018.\\n[93] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned\\nphotographs. In NeurIPS 2011, pages 1143–1151, 2011.\\n[94] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis\\nKalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting\\nlanguage and vision using crowdsourced dense image annotations. International Journal ofComputer Vision ,\\n123(1):32–73, 2017.', metadata={'source': 'OFA.pdf', 'page': 16}),\n",
       " Document(page_content='language and vision using crowdsourced dense image annotations. International Journal ofComputer Vision ,\\n123(1):32–73, 2017.\\n[95] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter:\\nElevating the role of image understanding in visual question answering. In Proceedings oftheIEEE Conference\\nonComputer Vision andPattern Recognition, pages 6904–6913, 2017.\\n[96] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and composi-\\ntional question answering. In CVPR 2019, pages 6700–6709, 2019.\\n[97] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth,\\nand Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications oftheACM , 59(2):64–73,\\n2016.\\n[98] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali,', metadata={'source': 'OFA.pdf', 'page': 16}),\n",
       " Document(page_content='2016.\\n[98] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali,\\nStefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4. International Journal\\nofComputer Vision, 128(7):1956–1981, 2020.\\n[99] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Ob-\\njects365: A large-scale, high-quality dataset for object detection. In Proceedings oftheIEEE/CVF International\\nConference onComputer Vision, pages 8430–8439, 2019.\\n[100] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,\\nAnish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv\\npreprint arXiv:2101.00027, 2020.\\n[101] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\\nCVPR 2016, pages 770–778, 2016.', metadata={'source': 'OFA.pdf', 'page': 16}),\n",
       " Document(page_content='preprint arXiv:2101.00027, 2020.\\n[101] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\\nCVPR 2016, pages 770–778, 2016.\\n[102] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR 2019, 2019.\\n[103] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth.\\nInECCV, 2016.\\n[104] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of\\nmachine translation. In Proceedings ofthe40th annual meeting oftheAssociation forComputational Linguistics ,\\npages 311–318, 2002.\\n[105] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation\\nwith human judgments. In Proceedings oftheaclworkshop onintrinsic andextrinsic evaluation measures for\\nmachine translation and/or summarization, pages 65–72, 2005.', metadata={'source': 'OFA.pdf', 'page': 16}),\n",
       " Document(page_content='with human judgments. In Proceedings oftheaclworkshop onintrinsic andextrinsic evaluation measures for\\nmachine translation and/or summarization, pages 65–72, 2005.\\n[106] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\\nevaluation. In Proceedings oftheIEEE conference oncomputer vision andpattern recognition , pages 4566–\\n4575, 2015.\\n[107] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image\\ncaption evaluation. In European conference oncomputer vision, pages 382–398. Springer, 2016.\\n[108] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In\\nProceedings oftheIEEE conference oncomputer vision andpattern recognition, pages 3128–3137, 2015.\\n[109] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by', metadata={'source': 'OFA.pdf', 'page': 16}),\n",
       " Document(page_content='[109] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by\\na two time-scale update rule converge to a local nash equilibrium. Advances inneural information processing\\nsystems, 30, 2017.\\n[110] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved\\ntechniques for training gans. Advances inneural information processing systems, 29:2234–2242, 2016.\\n17', metadata={'source': 'OFA.pdf', 'page': 16}),\n",
       " Document(page_content='[111] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence\\ntraining for image captioning. In Proceedings oftheIEEE conference oncomputer vision andpattern recognition ,\\npages 7008–7024, 2017.\\n[112] Guangxiang Zhao, Wenkai Yang, Xuancheng Ren, Lei Li, and Xu Sun. Well-classiﬁed examples are underesti-\\nmated in classiﬁcation with deep neural networks. CoRR, abs/2110.06537, 2021.\\n[113] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data\\naugmentation with a reduced search space. In Proceedings oftheIEEE/CVF Conference onComputer Vision\\nandPattern Recognition Workshops, pages 702–703, 2020.\\n[114] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In\\nProceedings oftheAAAI Conference onArtiﬁcial Intelligence, volume 34, pages 13001–13008, 2020.', metadata={'source': 'OFA.pdf', 'page': 17}),\n",
       " Document(page_content='Proceedings oftheAAAI Conference onArtiﬁcial Intelligence, volume 34, pages 13001–13008, 2020.\\n[115] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk min-\\nimization. In 6thInternational Conference onLearning Representations, ICLR 2018, Vancouver, BC,Canada,\\nApril 30-May 3,2018, Conference Track Proceedings. OpenReview.net, 2018.\\n[116] Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe. Cutmix:\\nRegularization strategy to train strong classiﬁers with localizable features. In 2019 IEEE/CVF International\\nConference onComputer Vision, ICCV 2019, Seoul, Korea (South), October 27-November 2,2019 , pages\\n6022–6031. IEEE, 2019.\\n[117] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta,\\nTheo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-ﬁltered 400 million\\nimage-text pairs. arXiv preprint arXiv:2111.02114, 2021.\\n18', metadata={'source': 'OFA.pdf', 'page': 17}),\n",
       " Document(page_content='A Implementation Details\\nA.1 Pretraining Datasets\\nWe construct pretraining datasets by incorporating Vision & Language data (i.e., image-text pairs), Vision data (i.e.,\\nraw image data, object-labeled data), and Language data (i.e., plain texts). For replication, the pretraining datasets are\\npublicly available. We carefully ﬁlter our pretraining data and exclude images that appear in the validation and test sets\\nof downstream tasks to avoid data leakage. The statistics on the pretraining datasets are listed in Table 11.\\nCross-modal Data For vision & language pretraining, we mainly apply image-text pairs, including image-caption\\npairs, image-QA pairs, and image-region pairs, as the pretraining data. For the pretraining tasks of image captioning\\nand image-text matching, we collect Conceptual Caption 12M (CC12M) [ 91], Conceptual Captions (CC3M) [ 92],\\nSBU [ 93], MSCOCO image captions (COCO) [ 74], and Visual Genome Captions (VG Captions) [ 94]. Speciﬁcally,', metadata={'source': 'OFA.pdf', 'page': 18}),\n",
       " Document(page_content='SBU [ 93], MSCOCO image captions (COCO) [ 74], and Visual Genome Captions (VG Captions) [ 94]. Speciﬁcally,\\nthe part of data from VG requires some additional processing. As texts in VG captions describe local regions on\\nthe images, we retrieve regions with area larger than 16,384pixels and construct region-caption pairs. For visual\\nquestion answering, we collect VQAv2 [ 95], VG-QA [ 94], as well as GQA [ 96]. VQAv2 is a visual question answering\\ndataset with real-world photographs from COCO. VG-QA is also a visual question answering dataset with real-world\\nphotographs from VG. The questions of VG-QA are related to speciﬁc regions on the images. GQA is a large VQA\\ndataset featuring compositional questions. The images of GQA are also collected from VG. For visual grounding\\nand grounded captioning, we collect data from RefCOCO [ 75], RefCOCO+ [ 75], RefCOCOg [ 76] and VG captions.', metadata={'source': 'OFA.pdf', 'page': 18}),\n",
       " Document(page_content='and grounded captioning, we collect data from RefCOCO [ 75], RefCOCO+ [ 75], RefCOCOg [ 76] and VG captions.\\nAdditional processing is applied to VG Captions for this task. Speciﬁcally, we use the data of VG that contains regions\\nwith area smaller than 16,384pixels for Visual Grounding, in order to encourage model to grasp ﬁne-grained alignments\\nbetween vision and language.\\nUni-modal Data Uni-modal data includes vision and language data. Vision data consists of raw images for image\\ninﬁlling and object-labeled images for object detection. For image inﬁlling, we collect raw images from OpenImages,\\nYFCC100M [ 97] and ImageNet-21K [ 81], and exclude annotations. Thus the model is unable to access labels in\\nthe pretraining stage. For object detection, we collect OpenImages [ 98], Object365 [ 99], VG and COCO for object\\ndetection. Language data consists of plain texts, i.e., passages consisting of sentences. We use around 140GB of data', metadata={'source': 'OFA.pdf', 'page': 18}),\n",
       " Document(page_content='detection. Language data consists of plain texts, i.e., passages consisting of sentences. We use around 140GB of data\\nfrom Pile [ 100] to leverage its diversity. Speciﬁcally, we extract natural language data and implement preprocessing\\nmethods, including truncation to the length of 512.\\nTable 11: Statistics on the datasets of pretraining tasks. “#Image” denotes the number of distinct images, and “#Sample”\\ndenotes the number of samples. *For language data, we report its storage following the previous studies [2, 28].\\nType Pretraining Task Source #Image #Sample\\nVision & LanguageImage CaptioningCC12M, CC3M, SBU, COCO, VG-Cap 14.78M 15.25MImage-Text Matching\\nVisual Question Answering VQAv2, VG-QA, GQA 178K 2.92M\\nVisual GroundingRefCOCO, RefCOCO+, RefCOCOg, VG-Cap 131K 3.20MGrounded Captioning\\nVisionDetection OpenImages, Object365, VG, COCO 2.98M 3.00M\\nImage Inﬁlling OpenImages, YFCC100M, ImageNet-21K 36.27M -\\nLanguage Masked Language Modeling Pile (Filtered) - 140GB*', metadata={'source': 'OFA.pdf', 'page': 18}),\n",
       " Document(page_content='VisionDetection OpenImages, Object365, VG, COCO 2.98M 3.00M\\nImage Inﬁlling OpenImages, YFCC100M, ImageNet-21K 36.27M -\\nLanguage Masked Language Modeling Pile (Filtered) - 140GB*\\nA.2 Pretraining Details\\nFor the image processing, we ﬁrst resize and crop the images into different resolutions, 256×256forOFA Tiny and\\nOFA Medium ,384×384forOFA Base,480×480forOFA Large andOFA Huge, with a ﬁxed patch size of 16×16. Note\\nthat training OFA Large andOFA Huge are time and computation consuming, we ﬁrst train them with images of the\\nresolution of 384×384and256×256, and continue pretraining with images of the resolution of 480×480.\\nFor each patch, we obtain its feature vector with the ﬁrst three blocks of ResNet [ 101]. The ResNet module is jointly\\ntrained along with the transformer module. Note that through extensive experiments we ﬁnd that random sampling\\npatches [ 47] does not bring additional beneﬁts in our scenario. For the text processing, we tokenize the texts with the\\n19', metadata={'source': 'OFA.pdf', 'page': 18}),\n",
       " Document(page_content='same BPE Tokenizer [ 63] as BART [ 31]. The maximum text sequence length of both encoder and decoder is set to 256.\\nWe share parameters between the embedding and the decoder softmax output layer.\\nFrom our preliminary experiments, we ﬁnd that the initialization for Transformer plays an important role. For OFA Base\\nandOFA Large , we initialize the transformer with most of the weights of BART Base andBART Large considering the\\nslight difference between OFA Transformer and BART as described in Sec 3.1. For OFA of the other sizes, we\\npretrain language models with the same pretraining strategy with BART and use the pretrained weights to initialize the\\nTransformer in OFA.\\nWe use the AdamW [ 102] optimizer with (β1,β2) = (0.9,0.999) andϵ= 1e-8to pretrain our models. We set the\\npeak learning rate to 2e-4, and apply a scheduler with linear decay with a warmup ratio of 0.01to control the learning', metadata={'source': 'OFA.pdf', 'page': 19}),\n",
       " Document(page_content='peak learning rate to 2e-4, and apply a scheduler with linear decay with a warmup ratio of 0.01to control the learning\\nrate. For regulation, we set dropout to 0.1and use weight decay with 0.01. We employ stochastic depth [ 103] with a\\n0.1rate (applied to encoder and decoder except for convolution blocks). We mix all the pretraining data within each\\nbatch, which contains 2,048vision&language samples, 256object detection samples, 256image-only samples and 512\\ntext-only samples. All models are pretrained for at least 300Ksteps except the models used for ablation study.\\nA.3 Details of Downstream Tasks\\nWe verify the capability of OFA on various downstream tasks in both ﬁnetuning and zero-shot settings. We design\\nvarious task-speciﬁc instructions to transfer the knowledge learned from pretraining to downstream tasks effectively.\\nThe instructions of different tasks are listed in Table 12. For ﬁnetuning, if not speciﬁed, the input image resolution is set', metadata={'source': 'OFA.pdf', 'page': 19}),\n",
       " Document(page_content='The instructions of different tasks are listed in Table 12. For ﬁnetuning, if not speciﬁed, the input image resolution is set\\nto480×480, and the other hyper-parameters remain the same as for pretraining. The experimental details of different\\ndownstream tasks, including both multimodal and uni-modal tasks, are listed below:\\nImage Captioning Image captioning is a standard vision&language task that requires models to generate an ap-\\npropriate and ﬂuent caption for an image. We adopt the most widely used MSCOCO Image Caption dataset [ 74]\\nto evaluate the multi-modal generation capability of OFA. We report BLEU-4 [ 104], METEOR [ 105], CIDEr [ 106],\\nand SPICE [ 107] scores on the Karpathy test split [ 108]. Following the previous standard practice, we ﬁrst ﬁnetune\\nOFA with cross-entropy loss for 2epochs with a batch size of 128and a learning rate of 1e−5, and label smoothing\\nis set to 0.1. We then ﬁnetune the model with CIDEr optimization for 3epochs with a batch size of 64, and disable', metadata={'source': 'OFA.pdf', 'page': 19}),\n",
       " Document(page_content='is set to 0.1. We then ﬁnetune the model with CIDEr optimization for 3epochs with a batch size of 64, and disable\\ndropout and stochastic depth. We report both scores at the two stages.\\nVisual Question Answering Visual question answering (VQA) is a cross-modal task that requires the models to\\nanswer the question given an image. Previous works such as VLMo [ 48] or SimVLM [ 22] deﬁne VQA as a classiﬁcation\\ntask. They use a linear output layer to predict the probability of each candidate answer on a given set. In contrast with\\nthese studies, to adapt the generative OFA model to VQA benchmark, we use the Trie-based search strategy mentioned\\nin Sec. 3.4 to ensure that the answer generated by OFA is constrained in the candidate set. We evaluate our model\\nwith other baselines on the commonly used VQAv2 dataset [ 95]. Accuracy scores on both test-dev and test-std sets\\nare reported. The OFA models of all the reported sizes are ﬁnetuned for 40,000steps with a batch size of 512. The', metadata={'source': 'OFA.pdf', 'page': 19}),\n",
       " Document(page_content='are reported. The OFA models of all the reported sizes are ﬁnetuned for 40,000steps with a batch size of 512. The\\nlearning rate is 5e−5with the label smoothing of 0.1. When ﬁnetuning OFA Large andOFA Huge, we increase the\\nimage resolution from 480to640. Linear interpolation of the image absolute positional embedding proposed in [ 6] is\\nemployed when transferring the pretrained OFA to VQA ﬁnetuning. During Trie-based searching, we constrain the\\ngenerated answers over the most frequent 3,129answer candidates. Exponential moving average (EMA) with decay\\nrate0.9999 is employed in ﬁnetuning.\\nVisual Entailment Visual entailment requires the model to evaluate how the given image and text are semantically\\ncorrelated, i.e., entailment, neutral, or contradiction. We perform experiments on the SNLI-VE dataset [ 73]. The image\\npremise, text premise and text hypothesis are fed to the encoder, and the decoder generates appropriate labels. To', metadata={'source': 'OFA.pdf', 'page': 19}),\n",
       " Document(page_content='premise, text premise and text hypothesis are fed to the encoder, and the decoder generates appropriate labels. To\\ntransfer the knowledge learned by pretraining to this task, we convert the labels entailment/neutral/contradiction to\\nyes/maybe/no. We also use the Trie-based search strategy to constrain the generated labels over the candidate set. We\\nreport accuracy on both dev and test sets. The OFA model is ﬁnetuned for 6epochs with a learning rate of 2e−5and a\\nbatch size of 256.\\nReferring Expression Comprehension Referring expression comprehension requires models to locate an image\\nregion described by a language query. Different from the approach taken by most previous methods [ 13,14] which\\nranks a set of candidate bounding boxes detected by a pretrained object detector, our method directly predicts the best\\nmatching bounding box without any proposals. We perform experiments on RefCOCO [ 75], RefCOCO+ [ 75], and', metadata={'source': 'OFA.pdf', 'page': 19}),\n",
       " Document(page_content='matching bounding box without any proposals. We perform experiments on RefCOCO [ 75], RefCOCO+ [ 75], and\\nRefCOCOg [ 76]. Consistent with other downstream tasks, we formulate referring expression comprehension as a\\nconditional sequence generation task. In detail, given an image and a language query, OFA generates the box sequence\\n(e.g.,⟨x1,y1,x2,y2⟩) in an autoregressive manner. We report the standard metric Acc@0.5 on the validation and test\\n20', metadata={'source': 'OFA.pdf', 'page': 19})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "final_data =text_splitter.split_documents(text_document_3[:20])\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "db = Chroma.from_documents(final_data[:50],OllamaEmbeddings(model=\"phi\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='OFA Base 41.0 30.9 138.2 24.2 42.8 31.7 146.7 25.8\\nOFA Large 42.4 31.5 142.2 24.5 43.6 32.2 150.7 26.2\\nOFA 43.9 31.8 145.3 24.8 44.9 32.5 154.9 26.6\\n4 Experiments\\nThis section provides experimental details and analyses to demonstrate our model’s effectiveness. See Appendix A for\\nimplementation details.\\n4.1 Results on Cross-modal Tasks\\nWe evaluate our models on different cross-modal downstream tasks, covering cross-modal understanding and generation.\\nSpeciﬁcally, we implement experiments on multimodal understanding datasets including VQAv2 for visual question\\nanswering and SNLI-VE [ 73] for visual entailment, and multimodal generation including MSCOCO Image Caption [ 74]\\nfor image captioning, RefCOCO / RefCOCO+ / RefCOCOg [ 75,76] for referring expression comprehension as this\\n6', metadata={'page': 5, 'source': 'OFA.pdf'})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is OFA?\"\n",
    "result = db.similarity_search(query)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OFA Base 41.0 30.9 138.2 24.2 42.8 31.7 146.7 25.8\\nOFA Large 42.4 31.5 142.2 24.5 43.6 32.2 150.7 26.2\\nOFA 43.9 31.8 145.3 24.8 44.9 32.5 154.9 26.6\\n4 Experiments\\nThis section provides experimental details and analyses to demonstrate our model’s effectiveness. See Appendix A for\\nimplementation details.\\n4.1 Results on Cross-modal Tasks\\nWe evaluate our models on different cross-modal downstream tasks, covering cross-modal understanding and generation.\\nSpeciﬁcally, we implement experiments on multimodal understanding datasets including VQAv2 for visual question\\nanswering and SNLI-VE [ 73] for visual entailment, and multimodal generation including MSCOCO Image Caption [ 74]\\nfor image captioning, RefCOCO / RefCOCO+ / RefCOCOg [ 75,76] for referring expression comprehension as this\\n6'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the context provided. think step by step before providing the answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}                                                                                                                             \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "llm = Ollama(model=\"phi\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "retreiver = db.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrival_chain = create_retrieval_chain(retreiver,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/ms/wgq4r1996sv7s24sg_nh2hm00000gn/T/ipykernel_16351/2800270473.py\", line 1, in <module>\n",
      "    result_1=retrival_chain.invoke({\"input\":\"OFA is a multimodal\"})\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2488, in invoke\n",
      "    callback_manager = get_callback_manager_for_config(config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 435, in get_callback_manager_for_config\n",
      "    return CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/ms/wgq4r1996sv7s24sg_nh2hm00000gn/T/ipykernel_16351/2800270473.py\", line 1, in <module>\n",
      "    result_1=retrival_chain.invoke({\"input\":\"OFA is a multimodal\"})\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2505, in invoke\n",
      "    input = step.invoke(input, config, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 469, in invoke\n",
      "    return self._call_with_config(self._invoke, input, config, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1585, in _call_with_config\n",
      "    callback_manager = get_callback_manager_for_config(config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 435, in get_callback_manager_for_config\n",
      "    return CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/ms/wgq4r1996sv7s24sg_nh2hm00000gn/T/ipykernel_16351/2800270473.py\", line 1, in <module>\n",
      "    result_1=retrival_chain.invoke({\"input\":\"OFA is a multimodal\"})\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2505, in invoke\n",
      "    input = step.invoke(input, config, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 469, in invoke\n",
      "    return self._call_with_config(self._invoke, input, config, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1599, in _call_with_config\n",
      "    context.run(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 456, in _invoke\n",
      "    **self.mapper.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3118, in invoke\n",
      "    callback_manager = CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 995, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 83, in _worker\n",
      "    work_item.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2488, in invoke\n",
      "    callback_manager = get_callback_manager_for_config(config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 435, in get_callback_manager_for_config\n",
      "    return CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 995, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 83, in _worker\n",
      "    work_item.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2505, in invoke\n",
      "    input = step.invoke(input, config, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3985, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1585, in _call_with_config\n",
      "    callback_manager = get_callback_manager_for_config(config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 435, in get_callback_manager_for_config\n",
      "    return CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 995, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 83, in _worker\n",
      "    work_item.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2507, in invoke\n",
      "    input = step.invoke(input, config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/retrievers.py\", line 196, in invoke\n",
      "    callback_manager = CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/ms/wgq4r1996sv7s24sg_nh2hm00000gn/T/ipykernel_16351/2800270473.py\", line 1, in <module>\n",
      "    result_1=retrival_chain.invoke({\"input\":\"OFA is a multimodal\"})\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2507, in invoke\n",
      "    input = step.invoke(input, config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 469, in invoke\n",
      "    return self._call_with_config(self._invoke, input, config, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1585, in _call_with_config\n",
      "    callback_manager = get_callback_manager_for_config(config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 435, in get_callback_manager_for_config\n",
      "    return CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/ms/wgq4r1996sv7s24sg_nh2hm00000gn/T/ipykernel_16351/2800270473.py\", line 1, in <module>\n",
      "    result_1=retrival_chain.invoke({\"input\":\"OFA is a multimodal\"})\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2507, in invoke\n",
      "    input = step.invoke(input, config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 469, in invoke\n",
      "    return self._call_with_config(self._invoke, input, config, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1599, in _call_with_config\n",
      "    context.run(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 456, in _invoke\n",
      "    **self.mapper.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3118, in invoke\n",
      "    callback_manager = CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 995, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 83, in _worker\n",
      "    work_item.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2488, in invoke\n",
      "    callback_manager = get_callback_manager_for_config(config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 435, in get_callback_manager_for_config\n",
      "    return CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 995, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 83, in _worker\n",
      "    work_item.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2505, in invoke\n",
      "    input = step.invoke(input, config, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 469, in invoke\n",
      "    return self._call_with_config(self._invoke, input, config, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1585, in _call_with_config\n",
      "    callback_manager = get_callback_manager_for_config(config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 435, in get_callback_manager_for_config\n",
      "    return CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 995, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 83, in _worker\n",
      "    work_item.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2505, in invoke\n",
      "    input = step.invoke(input, config, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 469, in invoke\n",
      "    return self._call_with_config(self._invoke, input, config, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1599, in _call_with_config\n",
      "    context.run(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 456, in _invoke\n",
      "    **self.mapper.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3118, in invoke\n",
      "    callback_manager = CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 995, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 83, in _worker\n",
      "    work_item.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3985, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1585, in _call_with_config\n",
      "    callback_manager = get_callback_manager_for_config(config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 435, in get_callback_manager_for_config\n",
      "    return CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 995, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 83, in _worker\n",
      "    work_item.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2507, in invoke\n",
      "    input = step.invoke(input, config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/prompts/base.py\", line 151, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1585, in _call_with_config\n",
      "    callback_manager = get_callback_manager_for_config(config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 435, in get_callback_manager_for_config\n",
      "    return CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 995, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 83, in _worker\n",
      "    work_item.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2507, in invoke\n",
      "    input = step.invoke(input, config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 276, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 633, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 764, in generate\n",
      "    CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 995, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 83, in _worker\n",
      "    work_item.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2507, in invoke\n",
      "    input = step.invoke(input, config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/output_parsers/base.py\", line 178, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1585, in _call_with_config\n",
      "    callback_manager = get_callback_manager_for_config(config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 435, in get_callback_manager_for_config\n",
      "    return CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n"
     ]
    }
   ],
   "source": [
    "result_1=retrival_chain.invoke({\"input\":\"OFA is a multimodal\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\n4.3 Loss Functions\\nIn the previous section, we introduced two key ideas for optimizing models in OFA: 1) using cross-modality attention and 2)\\npretraining on different tasks. However, these approaches are not directly translated into loss functions during training.\\nTo incorporate cross-modality information and tasks in OFA, we can design a loss function that includes the following three components: \\n4.3 Loss Functions\\nIn the previous section, we introduced two key ideas for optimizing models in OFA: 1) using cross-modality attention and 2)\\npretraining on different tasks. However, these approaches are not directly translated into loss functions during training. To incorporate\\ncross-modality information and tasks in OFA, we can design a loss function that includes the following three components: \\nQuestion: OFA is a multimodal                                                                                                                             \\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/ms/wgq4r1996sv7s24sg_nh2hm00000gn/T/ipykernel_16351/3302447647.py\", line 12, in <module>\n",
      "    result_1=chat_retriever_chain.invoke({\"input\":\"What is OFA?\"})\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/branch.py\", line 181, in invoke\n",
      "    callback_manager = get_callback_manager_for_config(config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 435, in get_callback_manager_for_config\n",
      "    return CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/ms/wgq4r1996sv7s24sg_nh2hm00000gn/T/ipykernel_16351/3302447647.py\", line 12, in <module>\n",
      "    result_1=chat_retriever_chain.invoke({\"input\":\"What is OFA?\"})\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/branch.py\", line 193, in invoke\n",
      "    expression_value = condition.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3985, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1585, in _call_with_config\n",
      "    callback_manager = get_callback_manager_for_config(config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 435, in get_callback_manager_for_config\n",
      "    return CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/ms/wgq4r1996sv7s24sg_nh2hm00000gn/T/ipykernel_16351/3302447647.py\", line 12, in <module>\n",
      "    result_1=chat_retriever_chain.invoke({\"input\":\"What is OFA?\"})\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/branch.py\", line 202, in invoke\n",
      "    output = runnable.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2488, in invoke\n",
      "    callback_manager = get_callback_manager_for_config(config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 435, in get_callback_manager_for_config\n",
      "    return CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/ms/wgq4r1996sv7s24sg_nh2hm00000gn/T/ipykernel_16351/3302447647.py\", line 12, in <module>\n",
      "    result_1=chat_retriever_chain.invoke({\"input\":\"What is OFA?\"})\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/branch.py\", line 202, in invoke\n",
      "    output = runnable.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2505, in invoke\n",
      "    input = step.invoke(input, config, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3985, in invoke\n",
      "    return self._call_with_config(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1585, in _call_with_config\n",
      "    callback_manager = get_callback_manager_for_config(config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 435, in get_callback_manager_for_config\n",
      "    return CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2009, in _configure\n",
      "    handler = LangChainTracer(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 91, in __init__\n",
      "    self.client = client or get_client()\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/tracers/langchain.py\", line 54, in get_client\n",
      "    _CLIENT = Client()\n",
      "              ^^^^^^^^\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 559, in __init__\n",
      "    _validate_api_key_if_hosted(self.api_url, self.api_key)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langsmith/client.py\", line 325, in _validate_api_key_if_hosted\n",
      "    raise ls_utils.LangSmithUserError(\n",
      "langsmith.utils.LangSmithUserError: API key must be provided when using hosted LangSmith API\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/ms/wgq4r1996sv7s24sg_nh2hm00000gn/T/ipykernel_16351/3302447647.py\", line 12, in <module>\n",
      "    result_1=chat_retriever_chain.invoke({\"input\":\"What is OFA?\"})\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4588, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/branch.py\", line 202, in invoke\n",
      "    output = runnable.invoke(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2507, in invoke\n",
      "    input = step.invoke(input, config)\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/retrievers.py\", line 196, in invoke\n",
      "    callback_manager = CallbackManager.configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1449, in configure\n",
      "    return _configure(\n",
      "  File \"/Users/aadityajain/AINLP/.venv/lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 2015, in _configure\n",
      "    logger.warning(\n",
      "Message: 'Unable to load requested LangChainTracer. To disable this warning, unset the LANGCHAIN_TRACING_V2 environment variables.'\n",
      "Arguments: (\"LangSmithUserError('API key must be provided when using hosted LangSmith API')\",)\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"phi\")\n",
    "\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain import hub\n",
    "\n",
    "rephrase_prompt = hub.pull(\"langchain-ai/chat-langchain-rephrase\")\n",
    "retriever = db.as_retriever()\n",
    "chat_retriever_chain = create_history_aware_retriever(\n",
    "    llm, retriever, rephrase_prompt\n",
    ")\n",
    "\n",
    "result_1=chat_retriever_chain.invoke({\"input\":\"What is OFA?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult_1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "result_1[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
